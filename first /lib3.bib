% Encoding: UTF-8
@misc{Course2019,
author = {Course, Email},
pages = {1--20},
title = {{No Title}},
url = {https://machinelearningmastery.com/horizontal-voting-ensemble/},
year = {2019}
}
@article{Golatkar2018,
abstract = {Breast Cancer is a major cause of death worldwide among women. Hematoxylin and Eosin (H{\&}E) stained breast tissue samples from biopsies are observed under microscopes for the primary diagnosis of breast cancer. In this paper, we propose a deep learning-based method for classification of H{\&}E stained breast tissue images released for BACH challenge 2018 by fine-tuning Inception-v3 convolutional neural network (CNN) proposed by Szegedy et al. These images are to be classified into four classes namely, i) normal tissue, ii) benign tumor, iii) in-situ carcinoma and iv) invasive carcinoma. Our strategy is to extract patches based on nuclei density instead of random or grid sampling, along with rejection of patches that are not rich in nuclei (non-epithelial) regions for training and testing. Every patch (nuclei-dense region) in an image is classified in one of the four above mentioned categories. The class of the entire image is determined using majority voting over the nuclear classes. We obtained an average four class accuracy of 85{\%} and an average two class (non-cancer vs. carcinoma) accuracy of 93{\%}, which improves upon a previous benchmark by Araujo et al.},
archivePrefix = {arXiv},
arxivId = {arXiv:1802.08080v2},
author = {Golatkar, Aditya and Anand, Deepak and Sethi, Amit},
doi = {10.1007/978-3-319-93000-8_95},
eprint = {arXiv:1802.08080v2},
isbn = {9783319929996},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Breast cancer,CNN,Deep learning,Histopathology,Transfer learning,majority voting},
mendeley-tags = {majority voting},
pages = {837--844},
title = {{Classification of Breast Cancer Histology Using Deep Learning}},
volume = {10882 LNCS},
year = {2018}
}
@article{Elfer2016,
abstract = {Real-time on-site histopathology review of biopsy tissues at the point-of-procedure has great potential for significant clinical value and improved patient care. For instance, on-site review can aid in rapid screening of diagnostic biopsies to reduce false-negative results, or in quantitative assessment of biospecimen quality to increase the efficacy of downstream laboratory and histopathology analysis. However, the only currently available rapid pathology method, frozen section analysis (FSA), is too time- and labor-intensive for use in screening large quantities of biopsy tissues and is too destructive for maximum tissue conservation in multiple small needle core biopsies. In this work we demonstrate the spectrally-compatible combination of the nuclear stain DRAQ5 and the anionic counterstain eosin as a dual-component fluorescent staining analog to hematoxylin and eosin intended for use on fresh, unsectioned tissues. Combined with optical sectioning fluorescence microscopy and pseudo-coloring algorithms, DRAQ5 and eosin ("D{\&}E") enables very fast, non-destructive psuedohistological imaging of tissues at the point-of-acquisition with minimal tissue handling and processing. D{\&}E was validated against H{\&}E on a one-to-one basis on formalin-fixed paraffin-embedded and frozen section tissues of various human organs using standard epi-fluorescence microscopy, demonstrating high fidelity of the staining mechanism as an H{\&}E analog. The method was then applied to fresh, whole 18G renal needle core biopsies and large needle core prostate biospecimen biopsies using fluorescence structured illumination optical sectioning microscopy. We demonstrate the ability to obtain high-resolution histology-like images of unsectioned, fresh tissues similar to subsequent H{\&}E staining of the tissue. The application of D{\&}E does not interfere with subsequent standard-of-care H{\&}E staining and imaging, preserving the integrity of the tissue for thorough downstream analysis. These results indicate that this dual-stain pseudocoloring method could provide a real-time histology-like image at the time of acquisition and valuable objective tissue analysis for the clinician at the time of service.},
author = {Elfer, Katherine N. and Sholl, Andrew B. and Wang, Mei and Tulman, David B. and Mandava, Sree H. and Lee, Benjamin R. and Brown, J. Quincy},
doi = {10.1371/journal.pone.0165530},
file = {:home/alkhaldieid/Desktop/papers/papers/medical papers/DRAQ5 and Eosin (‘D{\&}E') as an Analog to
Hematoxylin and Eosin for Rapid
Fluorescence Histology of Fresh Tissues.PDF:PDF},
issn = {19326203},
journal = {PLoS ONE},
number = {10},
pages = {1--18},
title = {{DRAQ5 and eosin ('D{\&}E') as an analog to hematoxylin and eosin for rapid fluorescence histology of fresh tissues}},
volume = {11},
year = {2016}
}
@article{Huang2018,
abstract = {Scaling up deep neural network capacity has been known as an effective approach to improving model quality for several different machine learning tasks. In many cases, increasing model capacity beyond the memory limit of a single accelerator has required developing special algorithms or infrastructure. These solutions are often architecture-specific and do not transfer to other tasks. To address the need for efficient and task-independent model parallelism, we introduce GPipe, a pipeline parallelism library that allows scaling any network that can be expressed as a sequence of layers. By pipelining different sub-sequences of layers on separate accelerators, GPipe provides the flexibility of scaling a variety of different networks to gigantic sizes efficiently. Moreover, GPipe utilizes a novel batch-splitting pipelining algorithm, resulting in almost linear speedup when a model is partitioned across multiple accelerators. We demonstrate the advantages of GPipe by training large-scale neural networks on two different tasks with distinct network architectures: (i) Image Classification: We train a 557-million-parameter AmoebaNet model and attain a top-1 accuracy of 84.4{\%} on ImageNet-2012, (ii) Multilingual Neural Machine Translation: We train a single 6-billion-parameter, 128-layer Transformer model on a corpus spanning over 100 languages and achieve better quality than all bilingual models.},
archivePrefix = {arXiv},
arxivId = {1811.06965},
author = {Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Mia Xu and Chen, Dehao and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V and Wu, Yonghui and Chen, Zhifeng},
eprint = {1811.06965},
pages = {1--11},
title = {{GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism}},
url = {http://arxiv.org/abs/1811.06965},
year = {2018}
}
@article{Baldominos2019,
abstract = {Neuroevolution is the field of study that uses evolutionary computation in order to optimize certain aspect of the design of neural networks, most often its topology and hyperparameters. The field was introduced in the late-1980s, but only in the latest years the field has become mature enough to enable the optimization of deep learning models, such as convolutional neural networks. In this paper, we rely on previous work to apply neuroevolution in order to optimize the topology of deep neural networks that can be used to solve the problem of handwritten character recognition. Moreover, we take advantage of the fact that evolutionary algorithms optimize a population of candidate solutions, by combining a set of the best evolved models resulting in a committee of convolutional neural networks. This process is enhanced by using specific mechanisms to preserve the diversity of the population. Additionally, in this paper, we address one of the disadvantages of neuroevolution: the process is very expensive in terms of computational time. To lessen this issue, we explore the performance of topology transfer learning: whether the best topology obtained using neuroevolution for a certain domain can be successfully applied to a different domain. By doing so, the expensive process of neuroevolution can be reused to tackle different problems, turning it into a more appealing approach for optimizing the design of neural networks topologies. After evaluating our proposal, results show that both the use of neuroevolved committees and the application of topology transfer learning are successful: committees of convolutional neural networks are able to improve classification results when compared to single models, and topologies learned for one problem can be reused for a different problem and data with a good performance. Additionally, both approaches can be combined by building committees of transferred topologies, and this combination attains results that combine the best of both approaches.},
author = {Baldominos, Alejandro and Saez, Yago and Isasi, Pedro},
doi = {10.1155/2019/2952304},
file = {:home/alkhaldieid/Desktop/papers/papers/transfer learning papers/Hybridizing Evolutionary Computation and Deep NeuralNetworks$\backslash$: An Approach to Handwriting Recognition UsingCommittees and Transfer Learning:},
issn = {10990526},
journal = {Complexity},
title = {{Hybridizing evolutionary computation and deep neural networks: An approach to handwriting recognition using committees and transfer learning}},
volume = {2019},
year = {2019}
}
@article{Koolen2014,
abstract = {Most standard algorithms for prediction with expert advice depend on a parameter called the learning rate. This learning rate needs to be large enough to fit the data well, but small enough to prevent overfitting. For the exponential weights algorithm, a sequence of prior work has established theoretical guarantees for higher and higher data-dependent tunings of the learning rate, which allow for increasingly aggressive learning. But in practice such theoretical tunings often still perform worse (as measured by their regret) than ad hoc tuning with an even higher learning rate. To close the gap between theory and practice we introduce an approach to learn the learning rate. Up to a factor that is at most (poly)logarithmic in the number of experts and the inverse of the learning rate, our method performs as well as if we would know the empirically best learning rate from a large range that includes both conservative small values and values that are much higher than those for which formal guarantees were previously available. Our method employs a grid of learning rates, yet runs in linear time regardless of the size of the grid.},
author = {Koolen, Wouter M. and {Van Erven}, Tim and Gr{\"{u}}nwald, Peter D.},
file = {:home/alkhaldieid/Desktop/papers/papers/Learning Rate Schedueling/5241-learning-the-learning-rate-for-prediction-with-expert-advice.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {January},
pages = {2294--2302},
title = {{Learning the learning rate for prediction with expert advice}},
volume = {3},
year = {2014}
}
@article{Sun2019a,
author = {Sun, Chuanzhi and Li, Chengtian and Liu, Yongmeng and Liu, Zewei and Wang, Xiaoming and Tan, Jiubin},
doi = {10.1109/access.2019.2941118},
file = {:home/alkhaldieid/Desktop/papers/papers/hyperparameter opt/08834779.pdf:pdf},
isbn = {6140003040},
journal = {IEEE Access},
pages = {132271--132278},
publisher = {IEEE},
title = {{Prediction Method of Concentricity and Perpendicularity of Aero Engine Multistage Rotors Based on PSO-BP Neural Network}},
volume = {7},
year = {2019}
}
@article{Xu2019,
abstract = {We present graph wavelet neural network (GWNN), a novel graph convolutional neural network (CNN), leveraging graph wavelet transform to address the shortcomings of previous spectral graph CNN methods that depend on graph Fourier transform. Different from graph Fourier transform, graph wavelet transform can be obtained via a fast algorithm without requiring matrix eigendecomposition with high computational cost. Moreover, graph wavelets are sparse and localized in vertex domain, offering high efficiency and good interpretability for graph convolution. The proposed GWNN significantly outperforms previous spectral graph CNNs in the task of graph-based semi-supervised classification on three benchmark datasets: Cora, Citeseer and Pubmed.},
annote = {a novel graph convolutional neural network (CNN), leveraging graph wavelet transform to address the short-We present graph wavelet neural network (GWNN), a novel graph convolutional neural network (CNN), leveraging graph wavelet transform to address the short- comings of previous spectral graph CNN methods that depend on graph Fourier
neural network (CNN), leveraging graph wavelet transform to address the short- comings of previous spectral graph CNN methods that depend on graph Fourier transform. Different from graph Fourier transform, graph wavelet transform can
comings of previous spectral graph CNN methods that depend on graph Fourier transform. Different from graph Fourier transform, graph wavelet transform can be obtained},
archivePrefix = {arXiv},
arxivId = {1904.07785},
author = {Xu, Bingbing and Shen, Huawei and Cao, Qi and Qiu, Yunqi and Cheng, Xueqi},
eprint = {1904.07785},
file = {:home/alkhaldieid/Desktop/papers/papers/future/RAPH WAVELET NEURAL NETWORK.pdf:pdf},
pages = {1--13},
title = {{Graph Wavelet Neural Network}},
url = {http://arxiv.org/abs/1904.07785},
year = {2019}
}
@article{Huang2017,
abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .},
archivePrefix = {arXiv},
arxivId = {arXiv:1608.06993v3},
author = {Huang, Gao and Liu, Zhuang and {Van Der Maaten}, Laurens and Weinberger, Kilian Q.},
doi = {10.1109/CVPR.2017.243},
eprint = {arXiv:1608.06993v3},
file = {:home/alkhaldieid/Desktop/papers/papers/Architectures/Original DenseNet paper.pdf:pdf},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {2261--2269},
title = {{Densely connected convolutional networks}},
volume = {2017-Janua},
year = {2017}
}
@article{Huangi2018,
abstract = {Batch Normalization (BN) is capable of accelerating the training of deep models by centering and scaling activations within mini-batches. In this work, we propose Decorrelated Batch Normalization (DBN), which not just centers and scales activations but whitens them. We explore multiple whitening techniques, and find that PCA whitening causes a problem we call stochastic axis swapping, which is detrimental to learning. We show that ZCA whitening does not suffer from this problem, permitting successful learning. DBN retains the desirable qualities of BN and further improves BN's optimization efficiency and generalization ability. We design comprehensive experiments to show that DBN can improve the performance of BN on multilayer perceptrons and convolutional neural networks. Furthermore, we consistently improve the accuracy of residual networks on CIFAR-10, CIFAR-100, and ImageNet.},
archivePrefix = {arXiv},
arxivId = {1804.08450},
author = {Huangi, Lei and Yang, Dawei and Lang, Bo and Deng, Jia},
doi = {10.1109/CVPR.2018.00089},
eprint = {1804.08450},
file = {:home/alkhaldieid/Desktop/papers/papers/BN/1804.08450.pdf:pdf;:home/alkhaldieid/Desktop/papers/papers/1804.08450.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {791--800},
title = {{Decorrelated Batch Normalization}},
year = {2018}
}
@article{Bach2017,
author = {Bach, Francis},
file = {:home/alkhaldieid/Desktop/papers/papers/p629-bach.pdf:pdf},
pages = {1--53},
title = {{Breaking the Curse of Dimensionality with Convex Neural Networks}},
volume = {18},
year = {2017}
}
@article{Shell2015,
archivePrefix = {arXiv},
arxivId = {suresh govindarajan},
author = {Shell, Michael},
doi = {10.1109/JSEN.2017.2719579},
eprint = {suresh govindarajan},
file = {:home/alkhaldieid/Desktop/papers/Samples/IEEEtran{\_}HOWTO.pdf:pdf},
isbn = {1097-4199 (Electronic)$\backslash$r0896-6273 (Linking)},
issn = {1530-437X},
number = {8},
pages = {1--28},
pmid = {18701059},
title = {{IEEE TEX guide}},
volume = {14},
year = {2015}
}
@article{Albarqouni2016,
abstract = {The lack of publicly available ground-truth data has been identified as the major challenge for transferring recent developments in deep learning to the biomedical imaging domain. Though crowdsourcing has enabled annotation of large scale databases for real world images, its application for biomedical purposes requires a deeper understanding and hence, more precise definition of the actual annotation task. The fact that expert tasks are being outsourced to non-expert users may lead to noisy annotations introducing disagreement between users. Despite being a valuable resource for learning annotation models from crowdsourcing, conventional machine-learning methods may have difficulties dealing with noisy annotations during training. In this manuscript, we present a new concept for learning from crowds that handle data aggregation directly as part of the learning process of the convolutional neural network (CNN) via additional crowdsourcing layer (AggNet). Besides, we present an experimental study on learning from crowds designed to answer the following questions. 1) Can deep CNN be trained with data collected from crowdsourcing? 2) How to adapt the CNN to train on multiple types of annotation datasets (ground truth and crowd-based)? 3) How does the choice of annotation and aggregation affect the accuracy? Our experimental setup involved Annot8, a self-implemented web-platform based on Crowdflower API realizing image annotation tasks for a publicly available biomedical image database. Our results give valuable insights into the functionality of deep CNN learning from crowd annotations and prove the necessity of data aggregation integration.},
author = {Albarqouni, Shadi and Baur, Christoph and Achilles, Felix and Belagiannis, Vasileios and Demirci, Stefanie and Navab, Nassir},
doi = {10.1109/TMI.2016.2528120},
file = {:home/alkhaldieid/Desktop/papers/papers/future/07405343.pdf:pdf},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Aggregation,crowdsourcing,deep learning,gamification,online learning},
number = {5},
pages = {1313--1321},
publisher = {IEEE},
title = {{AggNet: Deep Learning From Crowds for Mitosis Detection in Breast Cancer Histology Images}},
volume = {35},
year = {2016}
}
@article{Pellowe2008,
author = {Pellowe, Carol},
file = {:home/alkhaldieid/Desktop/papers/How to write a good paper/How to Write an Abstract.pdf:pdf},
issn = {14747359},
journal = {HIV Nursing},
number = {2},
pages = {18--19},
title = {{How to write an abstract}},
volume = {8},
year = {2008}
}
@article{Nanni2018,
abstract = {This work presents a system based on an ensemble of Convolutional Neural Networks (CNNs) and descriptors for bioimage classification that has been validated on different datasets of color images. The proposed system represents a very simple yet effective way of boosting the performance of trained CNNs by composing multiple CNNs into an ensemble and combining scores by sum rule. Several types of ensembles are considered, with different CNN topologies along with different learning parameter sets. The proposed system not only exhibits strong discriminative power but also generalizes well over multiple datasets thanks to the combination of multiple descriptors based on different feature types, both learned and handcrafted. Separate classifiers are trained for each descriptor, and the entire set of classifiers is combined by sum rule. Results show that the proposed system obtains state-of-the-art performance across four different bioimage and medical datasets. The MATLAB code of the descriptors will be available at https://github.com/LorisNanni.},
author = {Nanni, Loris and Ghidoni, Stefano and Brahnam, Sheryl},
doi = {10.1016/j.aci.2018.06.002},
file = {:home/alkhaldieid/Desktop/papers/papers/Ensemble/Ensemble of convolutional neural networks for bioimage classification.pdf:pdf},
issn = {22108327},
journal = {Applied Computing and Informatics},
title = {{Ensemble of convolutional neural networks for bioimage classification}},
year = {2018}
}
@book{Iba2018,
abstract = {{\textcopyright} Springer Nature Singapore Pte Ltd. 2018. All Rights Reserved. This book provides theoretical and practical knowledge about a methodology for evolutionary algorithm-based search strategy with the integration of several machine learning and deep learning techniques. These include convolutional neural networks, Gr{\"{o}}bner bases, relevance vector machines, transfer learning, bagging and boosting methods, clustering techniques (affinity propagation), and belief networks, among others. The development of such tools contributes to better optimizing methodologies. Beginning with the essentials of evolutionary algorithms and covering interdisciplinary research topics, the contents of this book are valuable for different classes of readers: novice, intermediate, and also expert readers from related fields. Following the chapters on introduction and basic methods, Chapter 3 details a new research direction, i.e., neuro-evolution, an evolutionary method for the generation of deep neural networks, and also describes how evolutionary methods are extended in combination with machine learning techniques. Chapter 4 includes novel methods such as particle swarm optimization based on affinity propagation (PSOAP), and transfer learning for differential evolution (TRADE), another machine learning approach for extending differential evolution. The last chapter is dedicated to the state of the art in gene regulatory network (GRN) research as one of the most interesting and active research fields. The author describes an evolving reaction network, which expands the neuro-evolution methodology to produce a type of genetic network suitable for biochemical systems and has succeeded in designing genetic circuits in synthetic biology. The author also presents real-world GRN application to several artificial intelligent tasks, proposing a framework of motion generation by GRNs (MONGERN), which evolves GRNs to operate a real humanoid robot.},
author = {Iba, Hitoshi},
booktitle = {Evolutionary Approach to Machine Learning and Deep Neural Networks},
doi = {10.1007/978-981-13-0200-8},
publisher = {Springer Singapore},
title = {{Evolutionary Approach to Machine Learning and Deep Neural Networks}},
year = {2018}
}
@article{Klenske2016,
abstract = {Many controlled systems suffer from unmodeled nonlinear effects that recur periodically over time. Model-free controllers generally cannot compensate these effects, and good physical models for such periodic dynamics are challenging to construct. We investigate nonparametric system identification for periodically recurring nonlinear effects. Within a Gaussian process (GP) regression framework, we use a locally periodic covariance function to shape the hypothesis space, which allows for a structured extrapolation that is not possible with more widely used covariance functions. We show that hyperparameter estimation can be performed online using the maximum a posteriori point estimate, which provides an accuracy comparable with sampling methods as soon as enough data to cover the periodic structure has been collected. It is also shown how the periodic structure can be exploited in the hyperparameter optimization. The predictions obtained from the GP model are then used in a model predictive control framework to correct the external effect. The availability of good continuous predictions allows control at a higher rate than that of the measurements. We show that the proposed approach is particularly beneficial for sampling times that are smaller than, but of the same order of magnitude as, the period length of the external effect. In experiments on a physical system, an electrically actuated telescope mount, this approach achieves a reduction of about 20{\%} in root mean square tracking error.},
author = {Klenske, Edgar D. and Zeilinger, Melanie N. and Sch{\"{o}}lkopf, Bernhard and Hennig, Philipp},
doi = {10.1109/TCST.2015.2420629},
file = {:home/alkhaldieid/Desktop/papers/papers/hyperparameter opt/07105398.pdf:pdf},
issn = {10636536},
journal = {IEEE Transactions on Control Systems Technology},
keywords = {Adaptive control,nonlinear control systems,optimal control},
number = {1},
pages = {110--121},
title = {{Gaussian Process-Based Predictive Control for Periodic Error Correction}},
volume = {24},
year = {2016}
}
@article{Zakerzadeh2014,
abstract = {The curse of dimensionality has remained a challenge for a wide variety of algorithms in data mining, clustering, classification and privacy. Recently, it was shown that an increasing dimensionality makes the data resistant to effective privacy. The theoretical results seem to suggest that the dimensionality curse is a fundamental barrier to privacy preservation. However, in practice, we show that some of the common properties of real data can be leveraged in order to greatly ameliorate the negative effects of the curse of dimensionality. In real data sets, many dimensions contain high levels of inter-attribute correlations. Such correlations enable the use of a process known as vertical fragmentation in order to decompose the data into vertical subsets of smaller dimensionality. An information-theoretic criterion of mutual information is used in the vertical decomposition process. This allows the use of an anonymization process, which is based on combining results from multiple independent fragments. We present a general approach which can be applied to the k-anonymity, l-diversity, and t-closeness models. In the presence of inter-attribute correlations, such an approach continues to be much more robust in higher dimensionality, without losing accuracy. We present experimental results illustrating the effectiveness of the approach. This approach is resilient enough to prevent identity, attribute, and membership disclosure attack.},
archivePrefix = {arXiv},
arxivId = {1401.1174},
author = {Zakerzadeh, Hessam and Aggrawal, Charu C. and Barker, Ken},
eprint = {1401.1174},
file = {:home/alkhaldieid/Desktop/papers/papers/1401.1174curse:1174curse},
month = {jan},
title = {{Towards Breaking the Curse of Dimensionality for High-Dimensional Privacy: An Extended Version}},
url = {http://arxiv.org/abs/1401.1174},
year = {2014}
}
@article{Langley2000,
abstract = {This essay gives advice to authors of papers on machine learning, although much of it carries over to other computational disciplines. The issues covered include the material that should appear in a well-balanced paper, factors that arise in different approaches to evaluation, and ways to improve a submission's ability to communicate ideas to its readers},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Langley, Pat},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/alkhaldieid/Desktop/papers/How to write a good paper/Crafting Papers on Machine Learning.pdf:pdf},
isbn = {1-55860-707-2},
issn = {1098-6596},
journal = {Machine Learning - International Workshop 2000},
pages = {6},
pmid = {25246403},
title = {{Crafting papers on machine learning}},
url = {http://www.cse.unsw.edu.au/{~}icml2002/craft.html},
year = {2000}
}
@misc{,
file = {:home/alkhaldieid/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - ijcst{\_}format.doc:doc},
title = {ijcst{\_}format}
}
@article{Lee,
abstract = {Automatic diagnosis of breast cancer is a challenge that promises more accessible healthcare. In this paper, we describe the process of predicting slide-level cancer metastasis with machine learning techniques. First, a whole slide image is split into smaller patches which are classified in pixel-level based on Deeplab v3+. Pixel-level classifiers are trained under auto hardmining process. Next, based on pixel-level results for whole slide images, slide-level heatmaps are generated and classified by rule-based criteria for negative, itc, micro and macro. Finally patient-level pN stages are determined by each individual slide-level predictions.},
author = {Lee, Sanghun and Cho, Joonyoung and Kim, Sun Woo},
file = {:home/alkhaldieid/Desktop/papers/papers/TBD/Camelyon17{\_}.pdf:pdf},
pages = {3--4},
title = {{Automatic Classification on Patient-Level Breast Cancer Metastases}}
}
@article{Xie2013b,
abstract = {Representation learning, especially which by using deep learning, has been widely applied in classification. However, how to use limited size of labeled data to achieve good classification performance with deep neural network, and how can the learned features further improve classification remain indefinite. In this paper, we propose Horizontal Voting Vertical Voting and Horizontal Stacked Ensemble methods to improve the classification performance of deep neural networks. In the ICML 2013 Black Box Challenge, via using these methods independently, Bing Xu achieved 3rd in public leaderboard, and 7th in private leaderboard; Jingjing Xie achieved 4th in public leaderboard, and 5th in private leaderboard.},
archivePrefix = {arXiv},
arxivId = {1306.2759},
author = {Xie, Jingjing and Xu, Bing and Chuang, Zhang},
eprint = {1306.2759},
file = {:home/alkhaldieid/Desktop/papers/papers/Ensemble/Horizontal and Vertical Ensemble with Deep Representation for Classification.pdf:pdf},
title = {{Horizontal and Vertical Ensemble with Deep Representation for Classification}},
url = {http://arxiv.org/abs/1306.2759},
year = {2013}
}
@article{Builders2004,
author = {Builders, Book and Chin, Beverly},
file = {:home/alkhaldieid/Desktop/papers/How to write a good paper/how to write a paper pres.pdf:pdf},
isbn = {0471431540},
title = {{How to Write a Great Research Paper}},
url = {http://www.amazon.com/dp/0471431540},
year = {2004}
}
@article{\newlabel,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Francisco, Alecsandro Roberto Lemos and Goyena, Rodrigo and Francisco, Alecsandro Roberto Lemos and Goyena, Rodrigo and Francisco, Alecsandro Roberto Lemos},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/alkhaldieid/Desktop/papers/papers/Architectures/resnet.pdf:pdf;:home/alkhaldieid/Desktop/papers/papers/optimizers/The Marginal Value of Adaptive Gradient Methodsin Machine Learning:;:home/alkhaldieid/Desktop/papers/papers/TBD/AUTOMATED DETECTION OF PROSTATE CANCER ONMULTIPARAMETRIC MRI USING DEEP NEURAL NETWORKSTRAINED ON SPATIAL COORDINATES AND PATHOLOGY OFBIOPSY CORES.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Journal of Chemical Information and Modeling},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{済無No Title No Title}},
volume = {53},
year = {2013}
}
@article{Coelho2018,
abstract = {Breast cancer is the most frequently diagnosed cancer and leading cause of cancer-related death among females worldwide. In this article, we investigate the applicability of densely connected convolu- tional neural networks to the problems of histology image classification and whole slide image segmentation in the area of computer-aided diag- noses for breast cancer. To this end, we study various approaches for transfer learning and apply them to the data set from the 2018 grand challenge on breast cancer histology images (BACH). Keywords:},
author = {Coelho, Paulo and Pereira, Ana and Salgado, Marta},
doi = {10.1007/978-3-319-93000-8},
file = {:home/alkhaldieid/Desktop/papers/papers/ICIAR papers/Paper360-Final.pdf:pdf},
isbn = {9783319930008},
keywords = {capsule endoscopy,deep learning,gastrointestinal bleeding,lesion detection,machine learning,u-net},
number = {June},
pages = {553--561},
title = {{A Deep Learning Approach for Red Endoscopies}},
url = {http://dx.doi.org/10.1007/978-3-319-93000-8{\_}103},
volume = {1},
year = {2018}
}
@article{Dietterich2000,
abstract = {Ensemble methods are learning algorithms that construct a set of classifiers and then classify new data points by taking a (weighted) vote of their predictions. The original ensemble method is Bayesian averaging, but more recent algorithms include error-correcting output coding, Bagging, and boosting. This paper reviews these methods and explains why ensembles can often perform better than any single classifier. Some previous studies comparing ensemble methods are reviewed, and some new experiments are presented to uncover the reasons that Adaboost does not overfit rapidly.},
author = {Dietterich, Thomas G.},
file = {:home/alkhaldieid/Desktop/papers/papers/Ensemble/mcs-ensembles.pdf:pdf},
isbn = {3540677046},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {average},
mendeley-tags = {average},
pages = {1--15},
title = {{Ensemble methods in machine learning}},
volume = {1857 LNCS},
year = {2000}
}
@misc{,
file = {:home/alkhaldieid/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - SAMPLE-REFERENCE-CITATION.doc:doc},
title = {{SAMPLE-REFERENCE-CITATION}}
}
@incollection{Bluche2017,
author = {Bluche, Th{\'{e}}odore and Kermorvant, Christopher and Ney, Hermann},
booktitle = {Handwriting: Recognition, Development and Analysis},
isbn = {9781536119572},
month = {jan},
pages = {113--148},
publisher = {Nova Science Publishers, Inc.},
title = {{How to design deep neural networks for handwriting recognition}},
year = {2017}
}

@Misc{Course2019a,
  author = {Course, Email},
  title  = {{No Title}},
  year   = {2019},
  pages  = {1--20},
  url    = {https://machinelearningmastery.com/horizontal-voting-ensemble/},
}

@Article{Huang2018a,
  author        = {Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Mia Xu and Chen, Dehao and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V. and Wu, Yonghui and Chen, Zhifeng},
  title         = {{GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism}},
  year          = {2018},
  pages         = {1--11},
  abstract      = {Scaling up deep neural network capacity has been known as an effective approach to improving model quality for several different machine learning tasks. In many cases, increasing model capacity beyond the memory limit of a single accelerator has required developing special algorithms or infrastructure. These solutions are often architecture-specific and do not transfer to other tasks. To address the need for efficient and task-independent model parallelism, we introduce GPipe, a pipeline parallelism library that allows scaling any network that can be expressed as a sequence of layers. By pipelining different sub-sequences of layers on separate accelerators, GPipe provides the flexibility of scaling a variety of different networks to gigantic sizes efficiently. Moreover, GPipe utilizes a novel batch-splitting pipelining algorithm, resulting in almost linear speedup when a model is partitioned across multiple accelerators. We demonstrate the advantages of GPipe by training large-scale neural networks on two different tasks with distinct network architectures: (i) Image Classification: We train a 557-million-parameter AmoebaNet model and attain a top-1 accuracy of 84.4{\%} on ImageNet-2012, (ii) Multilingual Neural Machine Translation: We train a single 6-billion-parameter, 128-layer Transformer model on a corpus spanning over 100 languages and achieve better quality than all bilingual models.},
  archiveprefix = {arXiv},
  arxivid       = {1811.06965},
  eprint        = {1811.06965},
  file          = {:home/alkhaldieid/Desktop/papers/papers/GPipe$\backslash$: Easy Scaling with Micro-Batch Pipeline.pdf:pdf},
  url           = {http://arxiv.org/abs/1811.06965},
}
@article{Amidi2018,
abstract = {During the past decade, with the significant progress of computational power as well as ever-rising data availability, deep learning techniques became increasingly popular due to their excellent performance on computer vision problems. The size of the Protein Data Bank (PDB) has increased more than 15-fold since 1999, which enabled the expansion of models that aim at predicting enzymatic function via their amino acid composition. Amino acid sequence, however, is less conserved in nature than protein structure and therefore considered a less reliable predictor of protein function. This paper presents EnzyNet, a novel 3D convolutional neural networks classifier that predicts the Enzyme Commission number of enzymes based only on their voxel-based spatial structure. The spatial distribution of biochemical properties was also examined as complementary information. The two-layer architecture was investigated on a large dataset of 63,558 enzymes from the PDB and achieved an accuracy of 78.4{\%} by exploiting only the binary representation of the protein shape. Code and datasets are available at https://github.com/shervinea/enzynet.},
archivePrefix = {arXiv},
arxivId = {1707.06017},
author = {Amidi, Afshine and Amidi, Shervine and Vlachakis, Dimitrios and Megalooikonomou, Vasileios and Paragios, Nikos and Zacharaki, Evangelia I.},
doi = {10.7717/peerj.4750},
eprint = {1707.06017},
file = {:home/alkhaldieid/Desktop/papers/papers/1707.06017.pdf:pdf},
issn = {21678359},
journal = {PeerJ},
keywords = {3D convolutional neural networks,Deep learning,EnzyNet,Enzyme classification},
number = {5},
pages = {1--11},
title = {{EnzyNet: Enzyme classification using 3D convolutional neural networks on spatial representation}},
volume = {2018},
year = {2018}
}
@article{KOMANDIG1970,
abstract = {The use of plastic cases and crates in the horticulture grows more and more abroad. The easy shaping of the material permits that such items can be designed as optimum for the fruits and for the cultivation operation. Taking the increased production of fruit- tree plantations fruiting from 1969-70 in account, the introduction and use of plastics cases and crates seems to be practical and justified in the country.},
author = {{KOMANDI G} and {ERDOS I}},
file = {:home/alkhaldieid/Desktop/papers/papers/hyperparameter opt/yogatama14.pdf:pdf},
issn = {00272914},
journal = {Muanyag Es Gumi/Plastics and Rubber},
number = {2},
pages = {48--50},
title = {{Muanyag Rekeszek Es Ladak a Kerteszet Szamara}},
volume = {7},
year = {1970}
}
@misc{,
title = {{How to Make the Most Out of Being a PhD Introvert}},
url = {https://www.nextscientist.com/benefits-phd-introvert/},
urldate = {2019-10-06}
}
@article{Rate2017,
author = {Rate, Constant Learning},
file = {:home/alkhaldieid/Desktop/papers/papers/Learning Rate Schedueling/Learning Rate Schedules and Adaptive Learning Rate Methods for Deep Learning.pdf:pdf},
pages = {1--10},
title = {{Learning Rate Schedules and Adaptive Learning Rate Methods for Deep Learning Time-Based Decay}},
year = {2017}
}
@article{Kingma2014,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy},
eprint = {1412.6980},
file = {:home/alkhaldieid/Desktop/papers/papers/optimizers/adam.pdf:pdf;:home/alkhaldieid/Desktop/papers/papers/optimizers/adamax.pdf:pdf},
pages = {1--15},
title = {{Adam: A Method for Stochastic Optimization}},
url = {http://arxiv.org/abs/1412.6980},
year = {2014}
}
@inproceedings{Dietterich2000a,
abstract = {Ensemble methods are learning algorithms that construct a set of classifiers and then classify new data points by taking a (weighted) vote of their predictions. The original ensemble method is Bayesian averaging, but more recent algorithms include error-correcting output coding, Bagging, and boosting. This paper reviews these methods and explains why ensembles can often perform better than any single classifier. Some previous studies comparing ensemble methods are reviewed, and some new experiments are presented to uncover the reasons that Adaboost does not overfit rapidly.},
author = {Dietterich, Thomas G.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
file = {:home/alkhaldieid/Desktop/papers/papers/Ensemble/mcs-ensembles.pdf:pdf},
isbn = {3540677046},
issn = {03029743},
pages = {1--15},
title = {{Ensemble methods in machine learning}},
volume = {1857 LNCS},
year = {2000}
}
@article{Szegedy2017,
abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge},
archivePrefix = {arXiv},
arxivId = {arXiv:1602.07261v2},
author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A.},
eprint = {arXiv:1602.07261v2},
file = {:home/alkhaldieid/Desktop/papers/papers/incep{\_}res.pdf:pdf},
journal = {31st AAAI Conference on Artificial Intelligence, AAAI 2017},
pages = {4278--4284},
title = {{Inception-v4, inception-ResNet and the impact of residual connections on learning}},
year = {2017}
}
@article{Nakisa2018,
abstract = {Recently, emotion recognition using low-cost wearable sensors based on electroencephalogram and blood volume pulse has received much attention. Long short-term memory (LSTM) networks, a special type of recurrent neural networks, have been applied successfully to emotion classification. However, the performance of these sequence classifiers depends heavily on their hyperparameter values, and it is important to adopt an efficient method to ensure the optimal values. To address this problem, we propose a new framework to automatically optimize LSTM hyperparameters using differential evolution (DE). This is the first systematic study of hyperparameter optimization in the context of emotion classification. In this paper, we evaluate and compare the proposed framework with other state-of-the-art hyperparameter optimization methods (particle swarm optimization, simulated annealing, random search, and tree of Parzen estimators) using a new dataset collected from wearable sensors. Experimental results demonstrate that optimizing LSTM hyperparameters significantly improve the recognition rate of four-quadrant dimensional emotions with a 14{\%} increase in accuracy. The best model based on the optimized LSTM classifier using the DE algorithm achieved 77.68{\%} accuracy. The results also showed that evolutionary computation algorithms, particularly DE, are competitive for ensuring optimized LSTM hyperparameter values. Although DE algorithm is computationally expensive, it is less complex and offers higher diversity in finding optimal solutions.},
author = {Nakisa, Bahareh and Rastgoo, Mohammad Naim and Rakotonirainy, Andry and Maire, Frederic and Chandran, Vinod},
doi = {10.1109/ACCESS.2018.2868361},
file = {:home/alkhaldieid/Desktop/papers/papers/hyperparameter opt/08453798.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Differential evolution,emotion recognition,hyperparameter optimization,long short term memory,wearable physiological sensors},
pages = {49325--49338},
publisher = {IEEE},
title = {{Long short term memory hyperparameter optimization for a neural network based emotion recognition framework}},
volume = {6},
year = {2018}
}
@article{Zoph2018,
abstract = {Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (the "NASNet search space") which enables transferability. In our experiments, we search for the best convolutional layer (or "cell") on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, named "NASNet architecture". We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, NASNet achieves 2.4{\%} error rate, which is state-of-the-art. On ImageNet, NASNet achieves, among the published works, state-of-the-art accuracy of 82.7{\%} top-1 and 96.2{\%} top-5 on ImageNet. Our model is 1.2{\%} better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28{\%} in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74{\%} top-1 accuracy, which is 3.1{\%} better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0{\%} achieving 43.1{\%} mAP on the COCO dataset.},
archivePrefix = {arXiv},
arxivId = {arXiv:1707.07012v4},
author = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V.},
doi = {10.1109/CVPR.2018.00907},
eprint = {arXiv:1707.07012v4},
file = {:home/alkhaldieid/Desktop/papers/papers/Architectures/Learning Transferable Architectures for Scalable Image Recognition nasnet.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {8697--8710},
title = {{Learning Transferable Architectures for Scalable Image Recognition}},
year = {2018}
}
@article{Hsueh2019b,
abstract = {Learning rate scheduler has been a critical issue in the deep neural network training. Several schedulers and methods have been proposed, including step decay scheduler, adaptive method, cosine scheduler and cyclical scheduler. This paper proposes a new scheduling method, named hyperbolic-tangent decay (HTD). We run experiments on several benchmarks such as: ResNet, Wide ResNet and DenseNet for CIFAR-10 and CIFAR-100 datasets, LSTM for PAMAP2 dataset, ResNet on ImageNet and Fashion-MNIST datasets. In our experiments, HTD outperforms step decay and cosine scheduler in nearly all cases, while requiring less hyperparameters than step decay, and more flexible than cosine scheduler. Code is available at https://github.com/BIGBALLON/HTD.},
archivePrefix = {arXiv},
arxivId = {arXiv:1806.01593v2},
author = {Hsueh, Bo Yang and Li, Wei and Wu, I. Chen},
doi = {10.1109/WACV.2019.00052},
eprint = {arXiv:1806.01593v2},
file = {:home/alkhaldieid/Desktop/papers/papers/Learning Rate Schedueling/lrs.pdf:pdf;:home/alkhaldieid/Desktop/papers/papers/Learning Rate Schedueling/lrs{\_}backup.pdf:pdf},
isbn = {9781728119755},
journal = {Proceedings - 2019 IEEE Winter Conference on Applications of Computer Vision, WACV 2019},
pages = {435--442},
title = {{Stochastic gradient descent with hyperbolic-tangent decay on classification}},
year = {2019}
}
@article{Ozturk2018,
abstract = {In this study, classification performance of histopathological images which are processed by pre-processing algorithms using convolutional neural network structure is examined. The images are divided into four different pre-processing classes with their original state and processed with three different techniques. These classes are; original, normal pre-processing, other normal pre-processing and over pre-processing. Histopathological images of these four classes include cancerous and non-cancerous image patches. For these image classes, cancer patch classification is done using the same convolutional neural network structure. In this view, pre-processing effects on the classification success of the convolutional neural network is examined. For the normal pre-processing algorithm, background noise reduction and cell enhancement are applied. For over pre-processing, thresholding and morphological operations are applied in addition to normal preprocessing operations. At the end of the experiments, the most successful classification results are produced with the normal pre-processing algorithms. This is why the meaningful features of the image are left for the CNN structure that automatically learns the feature. The over pre-processing algorithm removes most of these important features from the image.},
author = {{\"{O}}zt{\"{u}}rk, Şaban and Akdemir, Bayram},
doi = {10.1016/j.procs.2018.05.166},
file = {:home/alkhaldieid/Desktop/papers/papers/Preprossing/Effects of Histopathological Image Pre-processing on Convolutional Neural Networks.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {CNN,classification,convolutional neural networks,histopathological image,preprocessing},
number = {June},
pages = {396--403},
publisher = {Elsevier B.V.},
title = {{Effects of Histopathological Image Pre-processing on Convolutional Neural Networks}},
url = {https://doi.org/10.1016/j.procs.2018.05.166},
volume = {132},
year = {2018}
}
@article{Zhu2018,
abstract = {Classical approaches for estimating optical flow have achieved rapid progress in the last decade. However, most of them are too slow to be applied in real-time video analysis. Due to the great success of deep learning, recent work has focused on using CNNs to solve such dense prediction problems. In this paper, we investigate a new deep architecture, Densely Connected Convolutional Networks (DenseNet), to learn optical flow. This specific architecture is ideal for the problem at hand as it provides shortcut connections throughout the network, which leads to implicit deep supervision. We extend current DenseNet to a fully convolutional network to learn motion estimation in an unsupervised manner. Evaluation results on three standard benchmarks demonstrate that DenseNet is a better fit than other widely adopted CNN architectures for optical flow estimation.},
archivePrefix = {arXiv},
arxivId = {arXiv:1707.06316v1},
author = {Zhu, Yi and Newsam, Shawn},
doi = {10.1109/ICIP.2017.8296389},
eprint = {arXiv:1707.06316v1},
file = {:home/alkhaldieid/Desktop/papers/papers/Architectures/DenseNet for Optical flow.pdf:pdf},
isbn = {9781509021758},
issn = {15224880},
journal = {Proceedings - International Conference on Image Processing, ICIP},
keywords = {Convolutional neural network,Optical flow estimation,Unsupervised learning},
pages = {790--794},
title = {{DenseNet for dense flow}},
volume = {2017-Septe},
year = {2018}
}
@article{Tan2019,
abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.4{\%} top-1 / 97.1{\%} top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7{\%}), Flowers (98.8{\%}), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
archivePrefix = {arXiv},
arxivId = {1905.11946},
author = {Tan, Mingxing and Le, Quoc V.},
eprint = {1905.11946},
file = {:home/alkhaldieid/Desktop/papers/papers/EfficientNet{\_} Rethinking Model Scaling for Convolutional Neural Networks.pdf:pdf},
title = {{EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1905.11946},
year = {2019}
}
@article{Zhou2018,
abstract = {In this paper, we describe our method for generating the answers for questions based on medical images, in the ImageCLEF VQA-Med 2018 task [7] [5]. Firstly, we use some image enhancement methods like clipping and questions preprocessing methods like lemmatization. Secondly, we use Inception-Resnet-v2 model (CNN) to extract image features, and use Bi-LSTM model (RNN) to encode the questions. Finally, we concatenate the coded questions with the image features to generate the answers. Our result was ranked secondly based on the BLEU, WBSS and CBSS metrics for evaluating semantic similarity, which suggests that our method is effective for generating answers from medical images and related questions.},
author = {Zhou, Yangyang and Kang, Xin and Ren, Fuji},
file = {:home/alkhaldieid/Desktop/papers/papers/Architectures/Employing Inception-Resnet-v2 and Bi-LSTMfor Medical Domain Visual Question Answering.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
keywords = {Attention mechanism,Bi-LSTM,Inception-Resnet-v2,VQA-Med},
title = {{Employing Inception-Resnet-v2 and Bi-LSTM for medical domain visual question answering}},
volume = {2125},
year = {2018}
}
@article{Ahmad2019,
abstract = {Breast Cancer is a most common form of cancer among women and life taking disease around the globe. Histopathological imaging is one of the methods for cancer diagnosis where Pathologists examine tissue cells under different microscopic standards but disagree on the final decision. This is a tiresome task and for that reason, Deep Neural Networks are being used for the supervised classification. We have used Breast Histology dataset having 240 training and 20 test images for classification of the histology images among four classes, i.e. Normal, Benign, In-situ carcinoma and Invasive carcinoma. The dataset was preprocessed for proper classification. We have applied transfer learning based on AlexNet, GoogleNet, and ResNet that can classify images at multiple cellular and nuclei configurations. This approach has resulted in 85{\%} accuracy in case of ResNet as the highest among others and further research is being done to increase its efficiency and reduce the human dependency. The proposed design can also be enhanced for automation of other medical imaging methods.},
annote = {Breast cancer is one of the leading causes of mortality in women. Early detection and treatment are imperative for improving sur- vival rates, which have steadily increased in recent years as a result of more sophisticated computer-aided-diagnosis (CAD) systems.},
archivePrefix = {arXiv},
arxivId = {1802.09424},
author = {Ahmad, Hafiz Mughees and Ghuffar, Sajid and Khurshid, Khurram},
doi = {10.1109/IBCAST.2019.8667221},
eprint = {1802.09424},
isbn = {9781538677292},
journal = {Proceedings of 2019 16th International Bhurban Conference on Applied Sciences and Technology, IBCAST 2019},
keywords = {AlexNet,Breast Cancer,Deep learning,GoogleNet,ResNet,clinical importance of early diagnosis},
mendeley-tags = {clinical importance of early diagnosis},
pages = {328--332},
title = {{Classification of Breast Cancer Histology Images Using Transfer Learning}},
year = {2019}
}
@article{Shin2016,
abstract = {Remarkable progress has been made in image recognition, primarily due to the availability of large-scale annotated datasets and deep convolutional neural networks (CNNs). CNNs enable learning data-driven, highly representative, hierarchical image features from sufficient training data. However, obtaining datasets as comprehensively annotated as ImageNet in the medical imaging domain remains a challenge. There are currently three major techniques that successfully employ CNNs to medical image classification: training the CNN from scratch, using off-the-shelf pre-trained CNN features, and conducting unsupervised CNN pre-training with supervised fine-tuning. Another effective method is transfer learning, i.e., fine-tuning CNN models pre-trained from natural image dataset to medical image tasks. In this paper, we exploit three important, but previously understudied factors of employing deep convolutional neural networks to computer-aided detection problems. We first explore and evaluate different CNN architectures. The studied models contain 5 thousand to 160 million parameters, and vary in numbers of layers. We then evaluate the influence of dataset scale and spatial image context on performance. Finally, we examine when and why transfer learning from pre-trained ImageNet (via fine-tuning) can be useful. We study two specific computer-aided detection (CADe) problems, namely thoraco-abdominal lymph node (LN) detection and interstitial lung disease (ILD) classification. We achieve the state-of-the-art performance on the mediastinal LN detection, and report the first five-fold cross-validation classification results on predicting axial CT slices with ILD categories. Our extensive empirical evaluation, CNN model analysis and valuable insights can be extended to the design of high performance CAD systems for other medical imaging tasks.},
archivePrefix = {arXiv},
arxivId = {arXiv:1602.03409v1},
author = {Shin, Hoo Chang and Roth, Holger R. and Gao, Mingchen and Lu, Le and Xu, Ziyue and Nogues, Isabella and Yao, Jianhua and Mollura, Daniel and Summers, Ronald M.},
doi = {10.1109/TMI.2016.2528162},
eprint = {arXiv:1602.03409v1},
file = {:home/alkhaldieid/Desktop/papers/papers/transfer learning papers/Transfer Learning.pdf:pdf},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Biomedical imaging,computer aided diagnosis,image analysis,machine learning,neural networks},
number = {5},
pages = {1285--1298},
title = {{Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning}},
volume = {35},
year = {2016}
}
@article{Chollet2017,
abstract = {We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.},
author = {Chollet, Fran{\c{c}}ois},
doi = {10.1109/CVPR.2017.195},
file = {:home/alkhaldieid/Desktop/papers/papers/Architectures/Xception{\_} Deep Learning With Depthwise Separable Convolutions.pdf:pdf},
isbn = {9781538604571},
journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
pages = {1800--1807},
title = {{Xception: Deep learning with depthwise separable convolutions}},
volume = {2017-Janua},
year = {2017}
}
@article{Alkhaldi,
author = {Alkhaldi, Eid},
file = {:home/alkhaldieid/Desktop/Work/kile/ijcst1/ijcst.pdf:pdf},
pages = {1--2},
title = {{Optimized Heterogeneous Ensemble for Histological Image Classification}}
}
@article{He2016,
abstract = {The particle swarm optimization (PSO) is an optimization algorithm based on intelligent optimization. Parameters selection of PSO will play an important role in performance and efficiency of the algorithm. In this paper, the performance of PSO is analyzed when the control parameters vary, including particle number, accelerate constant, inertia weight and maximum limited velocity. And then PSO with dynamic parameters has been applied on the neural network training for gearbox fault diagnosis, the results with different parameters of PSO are compared and analyzed. At last some suggestions for parameters selection are proposed to improve the performance of PSO.},
author = {He, Yan and Ma, Wei Jin and Zhang, Ji Ping},
doi = {10.1051/matecconf/20166302019},
file = {:home/alkhaldieid/Desktop/papers/papers/hyperparameter opt/pso NN.pdf:pdf},
journal = {MATEC Web of Conferences},
number = {2016},
pages = {02019},
title = {{The Parameters Selection of PSO Algorithm influencing On performance of Fault Diagnosis}},
volume = {63},
year = {2016}
}
@article{Drahansky2016,
abstract = {Abstract Long-haul travel does not constitute an obstacle for tourists to travel and is fast gaining the attention of tourists in new and unique experiences. This study was conducted to identify the long-haul travel motivation by international tourists to Penang. A total of 400 respondents participated in this survey, conducted around the tourist attractions in Penang, using cluster random sampling. However, only 370 questionnaires were only used for this research. Data were analysed using SPSS software 22 version. The findings, ‘knowledge and novelty seeking' were the main push factors that drove long-haul travel by international tourists to Penang. Meanwhile, the main pull factor that attracts long- haul travel by international tourists to Penang was its ‘culture and history'. Additionally, there were partly direct and significant relationships between socio-demographic, trip characteristics and travel motivation (push factors and pull factors). Overall, this study identified the long-haul travel motivations by international tourists to Penang based on socio-demographic, trip characteristics and travel motivation and has indirectly helped in understanding the long-haul travel market particularly for Penang and Southeast Asia. This research also suggested for an effective marketing and promotion strategy in pro- viding useful information that is the key to attract international tourists to travel long distances. Keywords:},
author = {Drahansky, Martin and Paridah, M.t and Moradbak, Amin and Mohamed, A.Z Z and abdulwahab taiwo Owolabi, Folahan and Asniza, Mustapha and {Abdul Khalid}, Shawkataly H.P P and abdulwahab taiwo Owolabi, Folahan and Asniza, Mustapha and {Abdul Khalid}, Shawkataly H.P P},
doi = {http://dx.doi.org/10.5772/57353},
file = {:home/alkhaldieid/Desktop/papers/papers/quantum/Quantum Neural Machine Learning$\backslash$: Theory and Experiments:},
journal = {Intech},
keywords = {tourism},
number = {tourism},
pages = {13},
title = {{We are IntechOpen , the world ' s leading publisher of Open Access books Built by scientists , for scientists TOP 1 {\%}}},
url = {https://www.intechopen.com/books/advanced-biometric-technologies/liveness-detection-in-biometrics},
volume = {i},
year = {2016}
}
@article{Yi2019,
abstract = {Generative adversarial networks have gained a lot of attention in the computer vision community due to their capability of data generation without explicitly modelling the probability density function. The adversarial loss brought by the discriminator provides a clever way of incorporating unlabeled samples into training and imposing higher order consistency. This has proven to be useful in many cases, such as domain adaptation, data augmentation, and image-to-image translation. These properties have attracted researchers in the medical imaging community, and we have seen rapid adoption in many traditional and novel applications, such as image reconstruction, segmentation, detection, classification, and cross-modality synthesis. Based on our observations, this trend will continue and we therefore conducted a review of recent advances in medical imaging using the adversarial training scheme with the hope of benefiting researchers interested in this technique.},
archivePrefix = {arXiv},
arxivId = {arXiv:1809.07294v2},
author = {Yi, Xin and Walia, Ekta and Babyn, Paul},
doi = {10.1016/j.media.2019.101552},
eprint = {arXiv:1809.07294v2},
file = {:home/alkhaldieid/Desktop/papers/papers/gan/Generative Adversarial Network in Medical Imaging$\backslash$: A Review:},
issn = {13618415},
journal = {Medical Image Analysis},
keywords = {deep learning,generative adversarial network,generative model,medical imaging,review},
pages = {101552},
title = {{Generative adversarial network in medical imaging: A review}},
volume = {58},
year = {2019}
}
@article{Hsueh2019,
abstract = {Learning rate scheduler has been a critical issue in the deep neural network training. Several schedulers and methods have been proposed, including step decay scheduler, adaptive method, cosine scheduler and cyclical scheduler. This paper proposes a new scheduling method, named hyperbolic-tangent decay (HTD). We run experiments on several benchmarks such as: ResNet, Wide ResNet and DenseNet for CIFAR-10 and CIFAR-100 datasets, LSTM for PAMAP2 dataset, ResNet on ImageNet and Fashion-MNIST datasets. In our experiments, HTD outperforms step decay and cosine scheduler in nearly all cases, while requiring less hyperparameters than step decay, and more flexible than cosine scheduler. Code is available at https://github.com/BIGBALLON/HTD.},
archivePrefix = {arXiv},
arxivId = {arXiv:1806.01593v2},
author = {Hsueh, Bo Yang and Li, Wei and Wu, I. Chen},
doi = {10.1109/WACV.2019.00052},
eprint = {arXiv:1806.01593v2},
file = {:home/alkhaldieid/Desktop/papers/papers/Learning Rate Schedueling/lrs.pdf:pdf},
isbn = {9781728119755},
journal = {Proceedings - 2019 IEEE Winter Conference on Applications of Computer Vision, WACV 2019},
pages = {435--442},
title = {{Stochastic gradient descent with hyperbolic-tangent decay on classification}},
year = {2019}
}
@article{Correct2019,
abstract = {It's January 28th and I should be working on my paper submissions. So should you!},
author = {Correct, Approximately},
file = {:home/alkhaldieid/Desktop/papers/How to write a good paper/Heuristics for Scientific Writing (a Machine Learning Perspective) – Approximately Correct.pdf:pdf},
pages = {1--14},
title = {{Heuristics for Scientific Writing (a Machine Learning Perspective)}},
url = {http://approximatelycorrect.com/2018/01/29/heuristics-technical-scientific-writing-machine-learning-perspective/},
year = {2019}
}
@article{Oetiker2018,
author = {Oetiker, Tobias and Partl, Hubert and Hyna, Irene and Schlegl, Elisabeth},
file = {:home/alkhaldieid/Desktop/papers/papers/lshort.pdf:pdf},
pages = {139},
title = {{The Not So Short Introduction to L A T E X 2 $\epsilon$ Or L A T E X 2 $\epsilon$ in 139 minutes}},
url = {https://tobi.oetiker.ch/lshort/lshort.pdf},
year = {2018}
}
@article{Mapping2018,
author = {Mapping, Ixel Intensity and Unctions, Transf E R F},
file = {:home/alkhaldieid/Desktop/papers/papers/medical papers/HISTOPATHOLOGICAL IMAGE ANALYSIS USING image processing.pdf:pdf},
number = {4},
pages = {1--3},
title = {{I Mage P Rocessing Ii}},
volume = {3},
year = {2018}
}

@Article{Baldominos2019a,
  author   = {Baldominos, Alejandro and Saez, Yago and Isasi, Pedro},
  title    = {{Hybridizing evolutionary computation and deep neural networks: An approach to handwriting recognition using committees and transfer learning}},
  journal  = {Complexity},
  year     = {2019},
  volume   = {2019},
  issn     = {10990526},
  abstract = {Neuroevolution is the field of study that uses evolutionary computation in order to optimize certain aspect of the design of neural networks, most often its topology and hyperparameters. The field was introduced in the late-1980s, but only in the latest years the field has become mature enough to enable the optimization of deep learning models, such as convolutional neural networks. In this paper, we rely on previous work to apply neuroevolution in order to optimize the topology of deep neural networks that can be used to solve the problem of handwritten character recognition. Moreover, we take advantage of the fact that evolutionary algorithms optimize a population of candidate solutions, by combining a set of the best evolved models resulting in a committee of convolutional neural networks. This process is enhanced by using specific mechanisms to preserve the diversity of the population. Additionally, in this paper, we address one of the disadvantages of neuroevolution: the process is very expensive in terms of computational time. To lessen this issue, we explore the performance of topology transfer learning: whether the best topology obtained using neuroevolution for a certain domain can be successfully applied to a different domain. By doing so, the expensive process of neuroevolution can be reused to tackle different problems, turning it into a more appealing approach for optimizing the design of neural networks topologies. After evaluating our proposal, results show that both the use of neuroevolved committees and the application of topology transfer learning are successful: committees of convolutional neural networks are able to improve classification results when compared to single models, and topologies learned for one problem can be reused for a different problem and data with a good performance. Additionally, both approaches can be combined by building committees of transferred topologies, and this combination attains results that combine the best of both approaches.},
  doi      = {10.1155/2019/2952304},
  file     = {:home/alkhaldieid/Desktop/papers/papers/transfer learning papers/Hybridizing Evolutionary Computation and Deep NeuralNetworks$\backslash$: An Approach to Handwriting Recognition UsingCommittees and Transfer Learning:},
}
@article{Wu2018b,
abstract = {Adjusting the learning rate schedule in stochastic gradient methods is an important unresolved problem which requires tuning in practice. If certain parameters of the loss function such as smoothness or strong convexity constants are known, theoretical learning rate schedules can be applied. However, in practice, such parameters are not known, and the loss function of interest is not convex in any case. The recently proposed batch normalization reparametrization is widely adopted in most neural network architectures today because, among other advantages, it is robust to the choice of Lipschitz constant of the gradient in loss function, allowing one to set a large learning rate without worry. Inspired by batch normalization, we propose a general nonlinear update rule for the learning rate in batch and stochastic gradient descent so that the learning rate can be initialized at a high value, and is subsequently decreased according to gradient observations along the way. The proposed method is shown to achieve robustness to the relationship between the learning rate and the Lipschitz constant, and near-optimal convergence rates in both the batch and stochastic settings ({\$}O(1/T){\$} for smooth loss in the batch setting, and {\$}O(1/\backslashsqrt{\{}T{\}}){\$} for convex loss in the stochastic setting). We also show through numerical evidence that such robustness of the proposed method extends to highly nonconvex and possibly non-smooth loss function in deep learning problems.Our analysis establishes some first theoretical understanding into the observed robustness for batch normalization and weight normalization.},
archivePrefix = {arXiv},
arxivId = {1803.02865},
author = {Wu, Xiaoxia and Ward, Rachel and Bottou, L{\'{e}}on},
eprint = {1803.02865},
file = {:home/alkhaldieid/Desktop/papers/papers/Learning Rate Schedueling/WNGrad{\_} Learn the Learning Rate in Gradient Descent.pdf:pdf;:home/alkhaldieid/Desktop/papers/papers/Learning Rate Schedueling/WNGrad{\_} Learn the Learning Rate in Gradient Descent{\_}backup.pdf:pdf},
pages = {1--16},
title = {{WNGrad: Learn the Learning Rate in Gradient Descent}},
url = {http://arxiv.org/abs/1803.02865},
year = {2018}
}
@article{Alkhaldia,
author = {Alkhaldi, Eid},
file = {:home/alkhaldieid/Desktop/ijcst/main.pdf:pdf},
pages = {1--2},
title = {{Optimized Heterogeneous Ensemble for Histological Image Classification}}
}

@Article{Elfer2016a,
  author   = {Elfer, Katherine N and Sholl, Andrew B and Wang, Mei and Tulman, David B and Mandava, Sree H and Lee, Benjamin R and Brown, J Quincy},
  title    = {{DRAQ5 and eosin ('D{\&}E') as an analog to hematoxylin and eosin for rapid fluorescence histology of fresh tissues}},
  journal  = {PLoS ONE},
  year     = {2016},
  volume   = {11},
  number   = {10},
  pages    = {1--18},
  issn     = {19326203},
  abstract = {Real-time on-site histopathology review of biopsy tissues at the point-of-procedure has great potential for significant clinical value and improved patient care. For instance, on-site review can aid in rapid screening of diagnostic biopsies to reduce false-negative results, or in quantitative assessment of biospecimen quality to increase the efficacy of downstream laboratory and histopathology analysis. However, the only currently available rapid pathology method, frozen section analysis (FSA), is too time- and labor-intensive for use in screening large quantities of biopsy tissues and is too destructive for maximum tissue conservation in multiple small needle core biopsies. In this work we demonstrate the spectrally-compatible combination of the nuclear stain DRAQ5 and the anionic counterstain eosin as a dual-component fluorescent staining analog to hematoxylin and eosin intended for use on fresh, unsectioned tissues. Combined with optical sectioning fluorescence microscopy and pseudo-coloring algorithms, DRAQ5 and eosin ("D{\&}E") enables very fast, non-destructive psuedohistological imaging of tissues at the point-of-acquisition with minimal tissue handling and processing. D{\&}E was validated against H{\&}E on a one-to-one basis on formalin-fixed paraffin-embedded and frozen section tissues of various human organs using standard epi-fluorescence microscopy, demonstrating high fidelity of the staining mechanism as an H{\&}E analog. The method was then applied to fresh, whole 18G renal needle core biopsies and large needle core prostate biospecimen biopsies using fluorescence structured illumination optical sectioning microscopy. We demonstrate the ability to obtain high-resolution histology-like images of unsectioned, fresh tissues similar to subsequent H{\&}E staining of the tissue. The application of D{\&}E does not interfere with subsequent standard-of-care H{\&}E staining and imaging, preserving the integrity of the tissue for thorough downstream analysis. These results indicate that this dual-stain pseudocoloring method could provide a real-time histology-like image at the time of acquisition and valuable objective tissue analysis for the clinician at the time of service.},
  doi      = {10.1371/journal.pone.0165530},
}
@article{Kornblith2018,
abstract = {Transfer learning is a cornerstone of computer vision, yet little work has been done to evaluate the relationship between architecture and transfer. An implicit hypothesis in modern computer vision research is that models that perform better on ImageNet necessarily perform better on other vision tasks. However, this hypothesis has never been systematically tested. Here, we compare the performance of 16 classification networks on 12 image classification datasets. We find that, when networks are used as fixed feature extractors or fine-tuned, there is a strong correlation between ImageNet accuracy and transfer accuracy ({\$}r = 0.99{\$} and {\$}0.96{\$}, respectively). In the former setting, we find that this relationship is very sensitive to the way in which networks are trained on ImageNet; many common forms of regularization slightly improve ImageNet accuracy but yield penultimate layer features that are much worse for transfer learning. Additionally, we find that, on two small fine-grained image classification datasets, pretraining on ImageNet provides minimal benefits, indicating the learned features from ImageNet do not transfer well to fine-grained tasks. Together, our results show that ImageNet architectures generalize well across datasets, but ImageNet features are less general than previously suggested.},
archivePrefix = {arXiv},
arxivId = {1805.08974},
author = {Kornblith, Simon and Shlens, Jonathon and Le, Quoc V.},
eprint = {1805.08974},
file = {:home/alkhaldieid/Desktop/papers/papers/transfer learning papers/Do Better ImageNet Models Transfer Better?.pdf:pdf},
title = {{Do Better ImageNet Models Transfer Better?}},
url = {http://arxiv.org/abs/1805.08974},
year = {2018}
}
@article{Real2019,
abstract = {The effort devoted to hand-crafting neural network image classifiers has motivated the use of architecture search to discover them automatically. Although evolutionary algorithms have been repeatedly applied to neural network topologies, the image classifiers thus discovered have remained inferior to human-crafted ones. Here, we evolve an image classifier— AmoebaNet-A—that surpasses hand-designs for the first time. To do this, we modify the tournament selection evolutionary algorithm by introducing an age property to favor the younger genotypes. Matching size, AmoebaNet-A has comparable accuracy to current state-of-the-art ImageNet models discovered with more complex architecture-search methods. Scaled to larger size, AmoebaNet-A sets a new state-of-theart 83.9{\%} top-1 / 96.6{\%} top-5 ImageNet accuracy. In a controlled comparison against a well known reinforcement learning algorithm, we give evidence that evolution can obtain results faster with the same hardware, especially at the earlier stages of the search. This is relevant when fewer compute resources are available. Evolution is, thus, a simple method to effectively discover high-quality architectures.},
archivePrefix = {arXiv},
arxivId = {arXiv:1802.01548v7},
author = {Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V.},
doi = {10.1609/aaai.v33i01.33014780},
eprint = {arXiv:1802.01548v7},
file = {:home/alkhaldieid/Desktop/papers/papers/Regularized Evolution for Image Classifier Architecture Search.pdf:pdf},
issn = {2159-5399},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
pages = {4780--4789},
title = {{Regularized Evolution for Image Classifier Architecture Search}},
volume = {33},
year = {2019}
}
@article{HeL.;LongLR;AntaniS.andThoma2009,
author = {{He, L.; Long, LR; Antani, S. and Thoma}, GR.},
file = {:home/alkhaldieid/Desktop/papers/papers/medical papers/Histology Image Analysis for Carcinoma Detection and Grading.pdf:pdf},
keywords = {goal of histological image classification},
mendeley-tags = {goal of histological image classification},
pages = {272--287},
title = {{Computer Assisted Diagnosis in Histopathology.}},
volume = {3},
year = {2009}
}
@article{Huangi2018a,
abstract = {Batch Normalization (BN) is capable of accelerating the training of deep models by centering and scaling activations within mini-batches. In this work, we propose Decorrelated Batch Normalization (DBN), which not just centers and scales activations but whitens them. We explore multiple whitening techniques, and find that PCA whitening causes a problem we call stochastic axis swapping, which is detrimental to learning. We show that ZCA whitening does not suffer from this problem, permitting successful learning. DBN retains the desirable qualities of BN and further improves BN's optimization efficiency and generalization ability. We design comprehensive experiments to show that DBN can improve the performance of BN on multilayer perceptrons and convolutional neural networks. Furthermore, we consistently improve the accuracy of residual networks on CIFAR-10, CIFAR-100, and ImageNet.},
archivePrefix = {arXiv},
arxivId = {1804.08450},
author = {Huangi, Lei and Yang, Dawei and Lang, Bo and Deng, Jia},
doi = {10.1109/CVPR.2018.00089},
eprint = {1804.08450},
file = {:home/alkhaldieid/Desktop/papers/papers/BN/1804.08450.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {791--800},
title = {{Decorrelated Batch Normalization}},
year = {2018}
}
@article{Bonanno1992,
author = {Bonanno, Giacomo},
file = {:home/alkhaldieid/Desktop/papers/papers/Learning Rate Schedueling/LEARNING WITH RANDOM LEARNING RATES.pdf:pdf},
keywords = {1,beliefs,i n t r,if two decision nodes,in an extensive game,information,information sets capture the,is her turn to,move,notion of what a,o d u c,player knows when it,subgame-perfect equilibrium,t i o n,x and y},
pages = {153--176},
title = {{L {\~{}} R L {\~{}} R}},
year = {1992}
}
@article{Bardou2018,
abstract = {Breast cancer is one of the main causes of cancer death worldwide. The diagnosis of biopsy tissue with hematoxylin and eosin stained images is non-trivial and specialists often disagree on the final diagnosis. Computer-aided Diagnosis systems contribute to reduce the cost and increase the efficiency of this process. Conventional classification approaches rely on feature extraction methods designed for a specific problem based on field-knowledge. To overcome the many difficulties of the feature-based approaches, deep learning methods are becoming important alternatives. A method for the classification of hematoxylin and eosin stained breast biopsy images using Convolutional Neural Networks (CNNs) is proposed. Images are classified in four classes, normal tissue, benign lesion, in situ carcinoma and invasive carcinoma, and in two classes, carcinoma and non-carcinoma. The architecture of the network is designed to retrieve information at different scales, including both nuclei and overall tissue organization. This design allows the extension of the proposed system to whole-slide histology images. The features extracted by the CNN are also used for training a Support Vector Machine classifier. Accuracies of 77.8{\%} for four class and 83.3{\%} for carcinoma/non-carcinoma are achieved. The sensitivity of our method for cancer cases is 95.6{\%}.},
author = {Bardou, Dalal and Zhang, Kun and Ahmad, Sayed Mohammad},
doi = {10.1109/ACCESS.2018.2831280},
isbn = {1111111111},
issn = {21693536},
journal = {IEEE Access},
keywords = {Histology images,bag of words,convolutional neural networks,engineered features,locality constrained linear coding},
pages = {24680--24693},
title = {{Classification of Breast Cancer Based on Histology Images Using Convolutional Neural Networks}},
volume = {6},
year = {2018}
}

@Article{Xie2013,
  author        = {Xie, Jingjing and Xu, Bing and Chuang, Zhang},
  title         = {{Horizontal and Vertical Ensemble with Deep Representation for Classification}},
  year          = {2013},
  abstract      = {Representation learning, especially which by using deep learning, has been widely applied in classification. However, how to use limited size of labeled data to achieve good classification performance with deep neural network, and how can the learned features further improve classification remain indefinite. In this paper, we propose Horizontal Voting Vertical Voting and Horizontal Stacked Ensemble methods to improve the classification performance of deep neural networks. In the ICML 2013 Black Box Challenge, via using these methods independently, Bing Xu achieved 3rd in public leaderboard, and 7th in private leaderboard; Jingjing Xie achieved 4th in public leaderboard, and 5th in private leaderboard.},
  archiveprefix = {arXiv},
  arxivid       = {1306.2759},
  eprint        = {1306.2759},
  file          = {:home/alkhaldieid/Desktop/papers/papers/Ensemble/Horizontal and Vertical Ensemble with Deep Representation for Classification.pdf:pdf;:home/alkhaldieid/Desktop/papers/papers/Ensemble/Horizontal and Vertical Ensemble with Deep Representation for Classification{\_}backup.pdf:pdf},
  url           = {http://arxiv.org/abs/1306.2759},
}
@article{Dayan2017,
author = {Dayan, Peter and Koller, Daphne and Thrun, Sebastian and Olshausen, Bruno and Weiss, Yair},
file = {:home/alkhaldieid/Desktop/papers/How to write a good paper/Guidelines for Writing a Good NIPS Paper.pdf:pdf},
pages = {2--5},
title = {{Guidelines for Writing a Good NIPS Paper}},
year = {2017}
}
@article{Pimkin2018,
abstract = {In the last years, neural networks have proven to be a powerful framework for various image analysis problems. However, some application domains have specific limitations. Notably, digital pathology is an example of such fields due to tremendous image sizes and quite limited number of training examples available. In this paper, we adopt state-of-the-art convolutional neural networks (CNN) architectures for digital pathology images analysis. We propose to classify image patches to increase effective sample size and then to apply an ensembling technique to build prediction for the original images. To validate the developed approaches, we conducted experiments with $\backslash$textit{\{}Breast Cancer Histology Challenge{\}} dataset and obtained 90$\backslash${\%} accuracy for the 4-class tissue classification task.},
archivePrefix = {arXiv},
arxivId = {arXiv:1802.00947v1},
author = {Pimkin, Artem and Makarchuk, Gleb and Kondratenko, Vladimir and Pisov, Maxim and Krivov, Egor and Belyaev, Mikhail},
doi = {10.1007/978-3-319-93000-8_100},
eprint = {arXiv:1802.00947v1},
file = {:home/alkhaldieid/Desktop/papers/papers/Preprossing/preprocessing{\_}no{\_}good.pdf:pdf},
isbn = {9783319929996},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Convolutional networks,Digital pathology,Ensembles,resolution constraint},
mendeley-tags = {resolution constraint},
pages = {877--886},
title = {{Ensembling Neural Networks for Digital Pathology Images Classification and Segmentation}},
volume = {10882 LNCS},
year = {2018}
}
@article{Szegedy2016,
abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2{\%} top-1 and 5.6{\%} top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5{\%} top-5 error on the validation set (3.6{\%} error on the test set) and 17.3{\%} top-1 error on the validation set.},
archivePrefix = {arXiv},
arxivId = {arXiv:1512.00567v3},
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
doi = {10.1109/CVPR.2016.308},
eprint = {arXiv:1512.00567v3},
file = {:home/alkhaldieid/Desktop/papers/papers/Architectures/Rethinking the Inception Architecture for Computer Vision.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {2818--2826},
title = {{Rethinking the Inception Architecture for Computer Vision}},
volume = {2016-Decem},
year = {2016}
}
@article{Li2015,
abstract = {Convolutional autoencoders (CAEs) are unsupervised feature extractors for high-resolution images. In the pre-processing step, whitening transformation has widely been adopted to remove redundancy by making adjacent pixels less correlated. Pooling is a biologically inspired operation to reduce the resolution of feature maps and achieve spatial invariance in convolutional neural networks. Conventionally, pooling methods are mainly determined empirically in most previous work. Therefore, our main purpose is to study the relationship between whitening processing and pooling operations in convolutional autoencoders for image classification. We propose an adaptive pooling approach based on the concepts of information entropy to test the effect of whitening on pooling in different conditions. Experimental results on benchmark datasets indicate that the performance of pooling strategies is associated with the distribution of feature activations, which can be affected by whitening processing. This provides guidance for the selection of pooling methods in convolutional autoencoders and other convolutional neural networks.},
author = {Li, Zuhe and Fan, Yangyu and Liu, Weihua},
doi = {10.1186/s13634-015-0222-1},
file = {:home/alkhaldieid/Desktop/papers/papers/Li2015{\_}Article{\_}TheEffectOfWhiteningTransforma.pdf:pdf},
isbn = {1363401502221},
issn = {16876180},
journal = {Eurasip Journal on Advances in Signal Processing},
keywords = {Computer vision,Convolutional neural network,Deep learning,Image classification,Sparse autoencoder,Unsupervised learning},
number = {1},
pages = {1--11},
title = {{The effect of whitening transformation on pooling operations in convolutional autoencoders}},
volume = {2015},
year = {2015}
}
@article{Alturkistani2015,
abstract = {The history of histology indicates that there have been significant changes in the techniques used for histological staining through chemical, molecular biology assays and immunological techniques, collectively referred to as histochemistry. Early histologists used the readily available chemicals to prepare tissues for microscopic studies; these laboratory chemicals were potassium dichromate, alcohol and the mercuric chloride to harden cellular tissues. Staining techniques used were carmine, silver nitrate, Giemsa, Trichrome Stains, Gram Stain and Hematoxylin among others. The purpose of this research was to assess past and current literature reviews, as well as case studies, with the aim of informing ways in which histological stains have been improved in the modern age. Results from the literature review has indicated that there has been an improvement in histopathology and histotechnology in stains used. There has been a rising need for efficient, accurate and less complex staining procedures. Many stain procedures are still in use today, and many others have been replaced with new immunostaining, molecular, non-culture and other advanced staining techniques. Some staining methods have been abandoned because the chemicals required have been medically proven to be toxic. The case studies indicated that in modern histology a combination of different stain techniques are used to enhance the effectiveness of the staining process. Currently, improved histological stains, have been modified and combined with other stains to improve their effectiveness.},
author = {Alturkistani, Hani A and Tashkandi, Faris M and Mohammedsaleh, Zuhair M},
doi = {10.5539/gjhs.v8n3p72},
issn = {19169736},
journal = {Global journal of health science},
keywords = {histochemistry,histological staining,histology,histopathology},
number = {3},
pages = {72--79},
title = {{Histological Stains: A Literature Review and Case Study}},
volume = {8},
year = {2015}
}
@article{BhriguKLahkar2019,
author = {{Bhrigu K Lahkar}},
file = {:home/alkhaldieid/Desktop/papers/papers/Architectures/Inception.pdf:pdf},
isbn = {6103544947},
number = {3},
pages = {2019},
title = {{No 主観的健康感を中心とした在宅高齢者における 健康関連指標に関する共分散構造分析Title}},
volume = {23},
year = {2019}
}
@article{Feurer2015,
abstract = {The success of machine learning in a broad range of applications has led to an ever-growing demand for machine learning systems that can be used off the shelf by non-experts. To be effective in practice, such systems need to automatically choose a good algorithm and feature preprocessing steps for a new dataset at hand, and also set their respective hyperparameters. Recent work has started to tackle this automated machine learning (AutoML) problem with the help of efficient Bayesian optimization methods. In this work we introduce a robust new AutoML system based on scikit-learn (using 15 classifiers, 14 feature preprocessing methods, and 4 data preprocessing methods, giving rise to a structured hypothesis space with 110 hyperparameters). This system, which we dub auto-sklearn, improves on existing AutoML methods by automatically taking into account past performance on similar datasets, and by constructing ensembles from the models evaluated during the optimization. Our system won the first phase of the ongoing ChaLearn AutoML challenge, and our comprehensive analysis on over 100 diverse datasets shows that it substantially outperforms the previous state of the art in AutoML.We also demonstrate the performance gains due to each of our contributions and derive insights into the effectiveness of the individual components of auto-sklearn. 1},
author = {Feurer, Matthias and Klein, Aaron and Eggensperger, Katharina and Springenberg, Jost Tobias and Blum, Manuel and Hutter, Frank},
file = {:home/alkhaldieid/Desktop/papers/papers/15-NIPS-auto-sklearn-preprint.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {2962--2970},
title = {{Efficient and robust automated machine learning}},
volume = {2015-Janua},
year = {2015}
}
@article{Colopy2018,
abstract = {Gaussian process regression (GPR) provides a means to generate flexible personalized models of time series of patient vital signs. These models can perform useful clinical inference in ways that population-based models cannot. A challenge for the use of personalized models is that they must be amenable to a wide range of parameterizations, to accommodate the plausible physiology of any patient in the population. Additionally, optimal performance is typically achieved when models are regularized in light of the knowledge of the physiology of the individual patient. In this paper, we describe a method to build GP models with varying complexity (via covariance kernels) and regularization (via fixed priors over hyperparameters) on a patient-specific level, for the purpose of robust vital-sign forecasting. To this end, our results present evidence in support of two main hypotheses: 1) the use of patient-specific models can outperform population-based models for useful clinical tasks, such as vital-sign forecasting; and 2) the optimal values of (hyper)parameters of these models are best determined by sophisticated methods of optimization, due to high correlation between dimensions of the search space. The resulting models are sufficiently robust to inform clinicians of a patient's vital-sign trajectory and warn of imminent deterioration.},
author = {Colopy, Glen Wright and Roberts, Stephen J. and Clifton, David A.},
doi = {10.1109/JBHI.2017.2751509},
file = {:home/alkhaldieid/Desktop/papers/papers/hyperparameter opt/08226743.pdf:pdf},
issn = {21682194},
journal = {IEEE Journal of Biomedical and Health Informatics},
keywords = {Bayesian optimisation,Gaussian processes,forecasting,patient monitoring,statistical learning,time series analysis},
number = {2},
pages = {301--310},
publisher = {IEEE},
title = {{Bayesian Optimization of Personalized Models for Patient Vital-Sign Monitoring}},
volume = {22},
year = {2018}
}
@article{Smith2017,
abstract = {It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally find the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of fixed values achieves improved classification accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate "reasonable bounds" -- linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10 and CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.01186v6},
author = {Smith, Leslie N.},
doi = {10.1109/WACV.2017.58},
eprint = {arXiv:1506.01186v6},
file = {:home/alkhaldieid/Desktop/papers/papers/Learning Rate Schedueling/cyc.pdf:pdf},
isbn = {9781509048229},
journal = {Proceedings - 2017 IEEE Winter Conference on Applications of Computer Vision, WACV 2017},
number = {April},
pages = {464--472},
title = {{Cyclical learning rates for training neural networks}},
year = {2017}
}
@article{Brancati2019,
abstract = {Accurately identifying and categorizing cancer structures/sub-types in histological images is an important clinical task involving a considerable workload and a specific subspecialty of pathologists. Digitizing pathology is a current trend that provides large amounts of visual data allowing a faster and more precise diagnosis through the development of automatic image analysis techniques. Recent studies have shown promising results for the automatic analysis of cancer tissue by using deep learning strategies that automatically extract and organize the discriminative information from the data. This paper explores deep learning methods for the automatic analysis of Hematoxylin and Eosin stained histological images of breast cancer and lymphoma. In particular, a deep learning approach is proposed for two different use cases: the detection of invasive ductal carcinoma in breast histological images and the classification of lymphoma sub-types. Both use cases have been addressed by adopting a residual convolutional neural network that is part of a convolutional autoencoder network (i.e., FusionNet). The performances have been evaluated on the public datasets of digital histological images and have been compared with those obtained by using different deep neural networks (UNet and ResNet). Additionally, comparisons with the state of the art have been considered, in accordance with different deep learning approaches. The experimental results show an improvement of 5.06{\%} in F-measure score for the detection task and an improvement of 1.09{\%} in the accuracy measure for the classification task.},
author = {Brancati, Nadia and {De Pietro}, Giuseppe and Frucci, Maria and Riccio, Daniel},
doi = {10.1109/ACCESS.2019.2908724},
file = {:home/alkhaldieid/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - IEEE Xplore Full-Text PDF(2).pdf:pdf},
issn = {2169-3536},
journal = {IEEE Access},
keywords = {Breast cancer,Deep learning,Feature extraction,Hematoxylin-and-Eosin-stained-histological-images,Histological images,Image reconstruction,Task analysis,Training,Tumors,automatic image analysis techniques,biological organs,biological tissues,biomedical optical imaging,breast cancer,breast histological images,breast invasive ductal carcinoma detection,cancer,cancer structures-subtypes,cancer tissue,cellular biophysics,classification task,clinical importance of early diagnosis,convolutional autoencoder network,convolutional neural nets,deep learning,deep learning approach,deep neural networks,detection,detection task,digital histological images,feature extraction,image classification,learning (artificial intelligence),lymphoma multiclassification,lymphoma sub-types,mammography,medical image processing,multi-classification,residual convolutional neural network},
mendeley-tags = {clinical importance of early diagnosis},
pages = {44709--44720},
title = {{A Deep Learning Approach for Breast Invasive Ductal Carcinoma Detection and Lymphoma Multi-Classification in Histological Images}},
url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp={\&}arnumber=8678759 https://ieeexplore.ieee.org/document/8678759/},
volume = {7},
year = {2019}
}
@article{SriVenkateshwaraCollegeofEngineering.DepartmentofElectronicsandCommunicationEngineering2016,
abstract = {Scholarly {\&} Professional "IEEE Catalog Number: CFP16F77-ART." Annotation Electronics, Information Sciences, Computer Engineering, telecommunication engineering and Electrical Engineering are the essential disciplines in the field of Electronics and Computer engineering Their evolution relies on progress in all these complementary scientific and technological fields This conference provides an international forum for the exchange of ideas, discussions on research results and the presentation of theoretical and practical applications in these domains.},
author = {{Sri Venkateshwara College of Engineering. Department of Electronics and Communication Engineering} and {Institute of Electrical and Electronics Engineers. Bangalore Section} and {Institute of Electrical and Electronics Engineers. Bangalore Section. COM Chapter} and {Sri Venkateshwara College of Engineering} and {Institute of Electrical and Electronics Engineers}},
file = {:home/alkhaldieid/Desktop/papers/papers/Preprossing/07808140.pdf:pdf},
isbn = {9781509007745},
keywords = {cifar10,convolutional neural network,normalization,preprocessing,standardization,zca},
pages = {1778--1781},
title = {{2016 IEEE International Conference on Recent Trends in Electronics, Information {\&} Communication Technology (RTEICT) : proceedings : 20-21 May 2016, Bengaluru, India}},
year = {2016}
}
@article{2018,
author = {江小涓},
file = {:usr/share/doc/bash/bash.pdf:pdf},
journal = {經濟研究},
number = {1},
pages = {1--81},
title = {{No Title网络空间服务业: 效率、约束及发展前景* ———以体育和文化产业为例}},
year = {2018}
}

@Article{Ahmad2019a,
  author        = {Ahmad, Hafiz Mughees and Ghuffar, Sajid and Khurshid, Khurram},
  title         = {{Classification of Breast Cancer Histology Images Using Transfer Learning}},
  journal       = {Proceedings of 2019 16th International Bhurban Conference on Applied Sciences and Technology, IBCAST 2019},
  year          = {2019},
  pages         = {328--332},
  abstract      = {Breast Cancer is a most common form of cancer among women and life taking disease around the globe. Histopathological imaging is one of the methods for cancer diagnosis where Pathologists examine tissue cells under different microscopic standards but disagree on the final decision. This is a tiresome task and for that reason, Deep Neural Networks are being used for the supervised classification. We have used Breast Histology dataset having 240 training and 20 test images for classification of the histology images among four classes, i.e. Normal, Benign, In-situ carcinoma and Invasive carcinoma. The dataset was preprocessed for proper classification. We have applied transfer learning based on AlexNet, GoogleNet, and ResNet that can classify images at multiple cellular and nuclei configurations. This approach has resulted in 85{\%} accuracy in case of ResNet as the highest among others and further research is being done to increase its efficiency and reduce the human dependency. The proposed design can also be enhanced for automation of other medical imaging methods.},
  annote        = {Breast cancer is one of the leading causes of mortality in women. Early detection and treatment are imperative for improving sur- vival rates, which have steadily increased in recent years as a result of more sophisticated computer-aided-diagnosis (CAD) systems.},
  archiveprefix = {arXiv},
  arxivid       = {1802.09424},
  doi           = {10.1109/IBCAST.2019.8667221},
  eprint        = {1802.09424},
  file          = {:home/alkhaldieid/Desktop/papers/papers/ICIAR papers/Classification of breast cancer histology images
using transfer learning.pdf:pdf},
  isbn          = {9781538677292},
  keywords      = {AlexNet,Breast Cancer,Deep learning,GoogleNet,ResNet,clinical importance of early diagnosis},
  mendeley-tags = {clinical importance of early diagnosis},
}
@article{Sidhom2018,
abstract = {Deep learning is an area of artificial intelligence that has received much attention in the past few years due to both an increase in computational power with the increased use of graphics processing units (GPUs) for computational analyses and the performance of these class of algorithms on visual recognition tasks. They have found utility in applications ranging from image search to facial recognition for security and social media purposes. Their continued success has propelled their use across many new domains including the medical field, in areas of radiology and pathology in particular, as these fields are thought to be driven by visual recognition tasks. In this paper, we present an application of deep learning, termed "transfer learning", using ResNet50, a pre-trained convolutional neural network (CNN) to act as a "feature-detector" at various magnifications to identify low and high level features in digital pathology images of various breast lesions for the purpose of classifying them correctly into the labels of normal, benign, in-situ, or invasive carcinoma as provided in the ICIAR 2018 Breast Cancer Histology Challenge (BACH).},
author = {Sidhom, John-william and Baras, Alexander S},
doi = {10.1101/333773},
file = {:home/alkhaldieid/Desktop/papers/papers/transfer learning papers/Convolving Pre-Trained Convolutional Neural Networks at Various Magnifications to Extract Diagnostic Features for Digital Pathology.pdf:pdf},
journal = {bioRxiv Bioinformatics},
title = {{Convolving Pre-Trained Convolutional Neural Networks at Various Magnifications to Extract Diagnostic Features for Digital Pathology}},
url = {http://biorxiv.org/cgi/content/short/333773v1},
year = {2018}
}

@Article{Bonanno1992a,
  author   = {Bonanno, Giacomo},
  title    = {{L {\~{}} R L {\~{}} R}},
  year     = {1992},
  pages    = {153--176},
  file     = {:home/alkhaldieid/Desktop/papers/papers/Learning Rate Schedueling/LEARNING WITH RANDOM LEARNING RATES.pdf:pdf},
  keywords = {1,beliefs,i n t r,if two decision nodes,in an extensive game,information,information sets capture the,is her turn to,move,notion of what a,o d u c,player knows when it,subgame-perfect equilibrium,t i o n,x and y},
}
@article{Tzeng,
archivePrefix = {arXiv},
arxivId = {arXiv:1412.3474v1},
author = {Tzeng, Eric and Hoffman, Judy and Zhang, Ning and Berkeley, U C and Saenko, Kate and Lowell, Umass and Darrell, Trevor and Berkeley, U C},
eprint = {arXiv:1412.3474v1},
file = {:home/alkhaldieid/Desktop/papers/papers/domain.pdf:pdf},
title = {{Deep Domain Confusion : Maximizing for Domain Invariance}}
}
@article{Overview2019,
author = {Overview, I and Program, B Academic},
file = {:home/alkhaldieid/Desktop/papers/papers/Research Proposals - Plan or Approach {\_} ORSP.pdf:pdf},
pages = {8--9},
title = {{Research Proposals - Plan or Approach}},
year = {2019}
}
@article{Jegou2017,
abstract = {State-of-the-art approaches for semantic image segmentation are built on Convolutional Neural Networks (CNNs). The typical segmentation architecture is composed of (a) a downsampling path responsible for extracting coarse semantic features, followed by (b) an upsampling path trained to recover the input image resolution at the output of the model and, optionally, (c) a post-processing module (e.g. Conditional Random Fields) to refine the model predictions. Recently, a new CNN architecture, Densely Connected Convolutional Networks (DenseNets), has shown excellent results on image classification tasks. The idea of DenseNets is based on the observation that if each layer is directly connected to every other layer in a feed-forward fashion then the network will be more accurate and easier to train. In this paper, we extend DenseNets to deal with the problem of semantic segmentation. We achieve state-of-the-art results on urban scene benchmark datasets such as CamVid and Gatech, without any further post-processing module nor pretraining. Moreover, due to smart construction of the model, our approach has much less parameters than currently published best entries for these datasets. Code to reproduce the experiments is available here : https://github.com/SimJeg/FC-DenseNet/blob/master/train.py},
archivePrefix = {arXiv},
arxivId = {arXiv:1611.09326v2},
author = {Jegou, Simon and Drozdzal, Michal and Vazquez, David and Romero, Adriana and Bengio, Yoshua},
doi = {10.1109/CVPRW.2017.156},
eprint = {arXiv:1611.09326v2},
file = {:home/alkhaldieid/Desktop/papers/papers/Architectures/DenseNet Semantic Segmentation.pdf:pdf},
isbn = {9781538607336},
issn = {21607516},
journal = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
pages = {1175--1183},
title = {{The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation}},
volume = {2017-July},
year = {2017}
}
@article{Kohavi1995,
author = {Kohavi, R.},
file = {:home/alkhaldieid/Desktop/papers/papers/straf{\_}Kfold.pdf:pdf},
number = {0},
pages = {0--6},
title = {{A study of Cross-Validation}},
year = {1995}
}
@article{Cao2018,
abstract = {Breast cancer is one of the most common types of cancer and leading cancer-related death causes for women. In the context of ICIAR 2018 Grand Challenge on Breast Cancer Histology Images, we compare one handcrafted feature extractor and five transfer learning feature extractors based on deep learning. We find out that the deep learning networks pretrained on ImageNet have better performance than the popular handcrafted features used for breast cancer histology images. The best feature extractor achieves an average accuracy of 79.30{\%}. To improve the classification performance, a random forest dissimilarity based integration method is used to combine different feature groups together. When the five deep learning feature groups are combined, the average accuracy is improved to 82.90{\%} (best accuracy 85.00{\%}). When handcrafted features are combined with the five deep learning feature groups, the average accuracy is improved to 87.10{\%} (best accuracy 93.00{\%}).},
archivePrefix = {arXiv},
arxivId = {arXiv:1803.11241v1},
author = {Cao, Hongliu and Bernard, Simon and Heutte, Laurent and Sabourin, Robert},
doi = {10.1007/978-3-319-93000-8_88},
eprint = {arXiv:1803.11241v1},
file = {:home/alkhaldieid/Desktop/papers/papers/transfer learning papers/Improve the performance of transfer learningwithout fine-tuning.pdf:pdf},
isbn = {9783319929996},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Breast cancer,Deep learning,Dissimilarity,High dimensional low sample size,Multi-view,Random forest,Transfer learning,difficulty of histology images classification,iimportance of automated classification},
mendeley-tags = {difficulty of histology images classification,iimportance of automated classification},
pages = {779--787},
title = {{Improve the Performance of Transfer Learning Without Fine-Tuning Using Dissimilarity-Based Multi-view Learning for Breast Cancer Histology Images}},
volume = {10882 LNCS},
year = {2018}
}
@article{Yosinski2014,
abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.},
archivePrefix = {arXiv},
arxivId = {arXiv:1411.1792v1},
author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
eprint = {arXiv:1411.1792v1},
file = {:home/alkhaldieid/Desktop/papers/papers/transfer learning papers/How transferable are features in deep neuralnetworks.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {January},
pages = {3320--3328},
title = {{How transferable are features in deep neural networks?}},
volume = {4},
year = {2014}
}
@article{Bergstra2013,
abstract = {Many computer vision algorithms depend on configuration settings that are typically hand-tuned in the course of evaluating the algorithm for a particular data set. While such parameter tuning is often presented as being incidental to the algorithm, correctly setting these parameter choices is frequently critical to realizing a method's full potential. Compounding matters, these parameters often must be re-tuned when the algorithm is applied to a new problem domain, and the tuning process itself often depends on personal experience and intuition in ways that are hard to quantify or describe. Since the performance of a given technique depends on both the fundamental quality of the algorithm and the details of its tuning, it is sometimes difficult to know whether a given technique is genuinely better, or simply better tuned. In this work, we propose a meta-modeling approach to support automated hyperparameter optimization, with the goal of providing practical tools that replace hand-tuning with a reproducible and unbiased optimization process. Our approach is to expose the underlying expression graph of how a performance metric (e.g. classification accuracy on validation examples) is computed from hyperparameters that govern not only how individual processing steps are applied, but even which processing steps are included. A hyperparameter optimization algorithm transforms this graph into a program for optimizing that performance metric. Our approach yields state of the art results on three disparate computer vision problems: a face-matching verification task (LFW), a face identification task (PubFig83) and an object recognition task (CIFAR-10), using a single broad class of feed-forward vision architectures. Copyright 2013 by the author(s).},
author = {Bergstra, J. and Yamins, D. and Cox, D. D.},
file = {:home/alkhaldieid/Desktop/papers/papers/hyperparameter opt/bergstra13.pdf:pdf},
journal = {30th International Conference on Machine Learning, ICML 2013},
number = {PART 1},
pages = {115--123},
title = {{Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures}},
year = {2013}
}
@article{N.BrancatiandG.DePietroandM.FrucciandD.Riccio2019,
author = {Brancati, Nadia and {De Pietro}, Giuseppe and Frucci, Maria and Riccio, Daniel},
doi = {10.1109/ACCESS.2019.2908724},
file = {:home/alkhaldieid/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - IEEE Xplore Full-Text PDF.pdf:pdf},
issn = {2169-3536},
journal = {IEEE Access},
keywords = {Breast cancer,Deep learning,Feature extraction,Hematoxylin-and-Eosin-stained-histological-images,Histological images,Image reconstruction,Task analysis,Training,Tumors,automatic image analysis techniques,biological organs,biological tissues,biomedical optical imaging,breast cancer,breast histological images,breast invasive ductal carcinoma detection,cancer,cancer structures-subtypes,cancer tissue,cellular biophysics,classification task,convolutional autoencoder network,convolutional neural nets,deep learning,deep learning approach,deep neural networks,detection,detection task,digital histological images,feature extraction,image classification,learning (artificial intelligence),lymphoma multiclassification,lymphoma sub-types,mammography,medical image processing,multi-classification,residual convolutional neural network},
pages = {44709--44720},
title = {{A Deep Learning Approach for Breast Invasive Ductal Carcinoma Detection and Lymphoma Multi-Classification in Histological Images}},
url = {https://ieeexplore.ieee.org/stamp/stamp.jsp?tp={\&}arnumber=8678759 https://ieeexplore.ieee.org/document/8678759/},
volume = {7},
year = {2019}
}
@article{Safwan,
abstract = {In this manuscript, we present a deep learning based approach for detection and classification of medical conditions such as classification of breast cancer and grading of diabetic retinopathy and macular edema. The performance of a convo-lutional neural network is dependent on the architecture of the network, amount of training data and data pre-processing. Transfer learning is oft utilized in deep learning so as to counter the limited availability of high quality annotated data. Hence, we create an ensemble of pre-trained classifiers by making use of models with different topologies and data normalization schemes. In general, the variance associated with an ensemble of classifiers is lower compared to a single classifier and thus generalizes better on unseen data. An F1 score based model pruning tech-nique was utilized for deciding the optimal number of classifiers in the ensemble. The proposed technique was tested on two separate biomedical image challenges, namely the (1) classification of breast cancer from histology images [BACH-2018] and (2) grading of diabetic retinopathy and macular edema from fundus images [IDRiD-2018]. On the histology data, our technique was adjudged jointly as the top performing algorithm while for the task of diabetic retinopathy grading, the technique was declared as the 4 th best performing algorithm.},
author = {Safwan, Mohammed and {Saketh Chennamsetty}, Sai and Kori, Avinash and {Alex Kollerathu}, Varghese and Krishnamurthi, Ganapathy},
file = {:home/alkhaldieid/Desktop/papers/papers/ICIAR papers/Convolving Pre-Trained Convolutional Neural Networks at Various Magnifications to Extract Dignostic Features for Digital Pathology.pdf:pdf},
number = {Midl 2018},
pages = {1--14},
title = {{Classification of Breast Cancer and Grading of Diabetic Retinopathy {\&} Macular Edema using Ensemble of Pre-trained Convolutional Neural Networks}},
url = {https://openreview.net/pdf?id=HyifSbnif}
}
@techreport{Wolpert,
abstract = {This paper introduces stacked generalization, a scheme for minimizing the generalization error rate of one or more generalizers. Stacked generalization works by deducing the biases of the generalizer(s) with respect to a provided learning set. This deduction proceeds by generalizing in a second space whose inputs are (for example) the guesses of the original generalizers when taught with part of the learning set and trying to guess the rest of it, and whose output is (for example) the correct guess. When used with multiple generalizers, stacked generalization can be seen as a more sophisticated version of cross-validation, exploiting a strategy more sophisticated than cross-vali-dation's crude winner-takes-all for combining the individual generalizers. When used with a single generalizer, stacked generalization is a scheme for estimating (and then correcting for) the error of a generalizer which has been trained on a particular learning set and then asked a particular question. After introducing stacked generalization and justifying its use, this paper presents two numerical experiments. The first demonstrates how stacked generalization improves upon a set of separate generalizers for the NETtalk task of translating text to phonemes. The second demonstrates how stacked generalization improves the performance of a single surface-fitter. With the other experimental evidence in the literature, the usual arguments supporting cross-validation, and the abstract justifications presented in this paper, the conclusion is that for almost any real-world generalization problem one should use some version of stacked generalization to minimize the generalization error rate. This paper ends by discussing some of the variations of stacked generalization, and how it touches on other fields like chaos theory.},
author = {Wolpert, David H},
file = {:home/alkhaldieid/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wolpert - Unknown - STACKED GENERALIZATION.pdf:pdf},
keywords = {combining generalizers,cross-validation,error estimation and correction 3,generalization and induction,learning set pre-processing},
title = {{STACKED GENERALIZATION}}
}

@Article{Alturkistani2015a,
  author   = {Alturkistani, Hani A. and Tashkandi, Faris M. and Mohammedsaleh, Zuhair M.},
  title    = {{Histological Stains: A Literature Review and Case Study}},
  journal  = {Global journal of health science},
  year     = {2015},
  volume   = {8},
  number   = {3},
  pages    = {72--79},
  issn     = {19169736},
  abstract = {The history of histology indicates that there have been significant changes in the techniques used for histological staining through chemical, molecular biology assays and immunological techniques, collectively referred to as histochemistry. Early histologists used the readily available chemicals to prepare tissues for microscopic studies; these laboratory chemicals were potassium dichromate, alcohol and the mercuric chloride to harden cellular tissues. Staining techniques used were carmine, silver nitrate, Giemsa, Trichrome Stains, Gram Stain and Hematoxylin among others. The purpose of this research was to assess past and current literature reviews, as well as case studies, with the aim of informing ways in which histological stains have been improved in the modern age. Results from the literature review has indicated that there has been an improvement in histopathology and histotechnology in stains used. There has been a rising need for efficient, accurate and less complex staining procedures. Many stain procedures are still in use today, and many others have been replaced with new immunostaining, molecular, non-culture and other advanced staining techniques. Some staining methods have been abandoned because the chemicals required have been medically proven to be toxic. The case studies indicated that in modern histology a combination of different stain techniques are used to enhance the effectiveness of the staining process. Currently, improved histological stains, have been modified and combined with other stains to improve their effectiveness.},
  doi      = {10.5539/gjhs.v8n3p72},
  file     = {:home/alkhaldieid/Desktop/papers/papers/Histological Stains$\backslash$: A Literature Review and Case Study .pdf:pdf},
  keywords = {histochemistry,histological staining,histology,histopathology},
}

@Article{Alkhaldib,
  author = {Alkhaldi, Eid},
  title  = {{Optimized Heterogeneous Ensemble for Histological Image Classification}},
  pages  = {1--2},
  file   = {:home/alkhaldieid/Desktop/Work/kile/ijcst1/ijcst.pdf:pdf},
}
@article{Deng,
author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-jia and Li, Kai and Fei-fei, Li},
file = {:home/alkhaldieid/Desktop/papers/papers/imagenet{\_}cvpr09.pdf:pdf},
pages = {2--9},
title = {{ImageNet : A Large-Scale Hierarchical Image Database}}
}
,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Francisco, Alecsandro Roberto Lemos},
booktitle = {Journal of Chemical Information and Modeling},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/alkhaldieid/Desktop/papers/papers/Architectures/resnet.pdf:pdf;:home/alkhaldieid/Desktop/papers/papers/optimizers/The Marginal Value of Adaptive Gradient Methodsin Machine Learning:;:home/alkhaldieid/Desktop/papers/papers/TBD/AUTOMATED DETECTION OF PROSTATE CANCER ONMULTIPARAMETRIC MRI USING DEEP NEURAL NETWORKSTRAINED ON SPATIAL COORDINATES AND PATHOLOGY OFBIOPSY CORES.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{済無No Title No Title}},
volume = {53},
year = {2013}
}
@article{Komura2018b,
abstract = {Abundant accumulation of digital histopathological images has led to the increased demand for their analysis, such as computer-aided diagnosis using machine learning techniques. However, digital pathological images and related tasks have some issues to be considered. In this mini-review, we introduce the application of digital pathological image analysis using machine learning algorithms, address some problems specific to such analysis, and propose possible solutions.},
author = {Komura, Daisuke and Ishikawa, Shumpei},
doi = {10.1016/j.csbj.2018.01.001},
file = {:home/alkhaldieid/Desktop/papers/papers/general histology papers/Machine Learning Methods for Histopathological Image Analysis.pdf:pdf},
issn = {20010370},
journal = {Computational and Structural Biotechnology Journal},
keywords = {Computer assisted diagnosis,Deep learning,Digital image analysis,Histopathology,Machine learning,Whole slide images},
pages = {34--42},
title = {{Machine Learning Methods for Histopathological Image Analysis}},
volume = {16},
year = {2018}
}
@article{Tajbakhsh2016,
abstract = {Training a deep convolutional neural network (CNN) from scratch is difficult because it requires a large amount of labeled training data and a great deal of expertise to ensure proper convergence. A promising alternative is to fine-tune a CNN that has been pre-trained using, for instance, a large set of labeled natural images. However, the substantial differences between natural and medical images may advise against such knowledge transfer. In this paper, we seek to answer the following central question in the context of medical image analysis: Can the use of pre-trained deep CNNs with sufficient fine-tuning eliminate the need for training a deep CNN from scratch? To address this question, we considered four distinct medical imaging applications in three specialties (radiology, cardiology, and gastroenterology) involving classification, detection, and segmentation from three different imaging modalities, and investigated how the performance of deep CNNs trained from scratch compared with the pre-trained CNNs fine-tuned in a layer-wise manner. Our experiments consistently demonstrated that 1) the use of a pre-trained CNN with adequate fine-tuning outperformed or, in the worst case, performed as well as a CNN trained from scratch; 2) fine-tuned CNNs were more robust to the size of training sets than CNNs trained from scratch; 3) neither shallow tuning nor deep tuning was the optimal choice for a particular application; and 4) our layer-wise fine-tuning scheme could offer a practical way to reach the best performance for the application at hand based on the amount of available data.},
archivePrefix = {arXiv},
arxivId = {arXiv:1706.00712v1},
author = {Tajbakhsh, Nima and Shin, Jae Y. and Gurudu, Suryakanth R. and Hurst, R. Todd and Kendall, Christopher B. and Gotway, Michael B. and Liang, Jianming},
doi = {10.1109/TMI.2016.2535302},
eprint = {arXiv:1706.00712v1},
file = {:home/alkhaldieid/Desktop/papers/papers/transfer learning papers/Full Training or Fine Tuning.pdf:pdf},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Carotid intima-media thickness,computer-aided detection,convolutional neural networks,deep learning,fine-tuning,medical image analysis,polyp detection,pulmonary embolism detection,video quality assessment},
number = {5},
pages = {1299--1312},
title = {{Convolutional Neural Networks for Medical Image Analysis: Full Training or Fine Tuning?}},
volume = {35},
year = {2016}
}
@book{Ye2017,
abstract = {In this paper, we propose a new automatic hyperparameter selection approach for determining the optimal network configuration (network structure and hyperparameters) for deep neural networks using particle swarm optimization (PSO) in combination with a steepest gradient descent algorithm. In the proposed approach, network configurations were coded as a set of real-number m-dimensional vectors as the individuals of the PSO algorithm in the search procedure. During the search procedure, the PSO algorithm is employed to search for optimal network configurations via the particles moving in a finite search space, and the steepest gradient descent algorithm is used to train the DNN classifier with a few training epochs (to find a local optimal solution) during the population evaluation of PSO. After the optimization scheme, the steepest gradient descent algorithm is performed with more epochs and the final solutions (pbest and gbest) of the PSO algorithm to train a final ensemble model and individual DNN classifiers, respectively. The local search ability of the steepest gradient descent algorithm and the global search capabilities of the PSO algorithm are exploited to determine an optimal solution that is close to the global optimum. We constructed several experiments on hand-written characters and biological activity prediction datasets to show that the DNN classifiers trained by the network configurations expressed by the final solutions of the PSO algorithm, employed to construct an ensemble model and individual classifier, outperform the random approach in terms of the generalization performance. Therefore, the proposed approach can be regarded an alternative tool for automatic network structure and parameter selection for deep neural networks.},
annote = {network structure and hyperparameter configuration play important roles in the training phase

an individual classifier (network) employed to predict results on a large-scale and high- dimensional dataset has several limitations, including weak generalization ability and instabil- ity in the training phase.},
author = {Ye, Fei},
booktitle = {PLoS ONE},
doi = {10.6084/m9.figshare.5624797.v1},
file = {:home/alkhaldieid/Desktop/papers/papers/hyperparameter opt/PSO-based automatic parameter selection for deep neural networks and its applications in large-scale and high-dimensional data.pdf:pdf},
isbn = {1111111111},
issn = {19326203},
number = {12},
pages = {1--36},
title = {{Particle swarm optimization-based automatic parameter selection for deep neural networks and its applications in large-scale and high-dimensional data}},
volume = {12},
year = {2017}
}

@Article{Golatkar2018a,
  author        = {Golatkar, Aditya and Anand, Deepak and Sethi, Amit},
  title         = {{Classification of Breast Cancer Histology Using Deep Learning}},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2018},
  volume        = {10882 LNCS},
  pages         = {837--844},
  issn          = {16113349},
  abstract      = {Breast Cancer is a major cause of death worldwide among women. Hematoxylin and Eosin (H{\&}E) stained breast tissue samples from biopsies are observed under microscopes for the primary diagnosis of breast cancer. In this paper, we propose a deep learning-based method for classification of H{\&}E stained breast tissue images released for BACH challenge 2018 by fine-tuning Inception-v3 convolutional neural network (CNN) proposed by Szegedy et al. These images are to be classified into four classes namely, i) normal tissue, ii) benign tumor, iii) in-situ carcinoma and iv) invasive carcinoma. Our strategy is to extract patches based on nuclei density instead of random or grid sampling, along with rejection of patches that are not rich in nuclei (non-epithelial) regions for training and testing. Every patch (nuclei-dense region) in an image is classified in one of the four above mentioned categories. The class of the entire image is determined using majority voting over the nuclear classes. We obtained an average four class accuracy of 85{\%} and an average two class (non-cancer vs. carcinoma) accuracy of 93{\%}, which improves upon a previous benchmark by Araujo et al.},
  archiveprefix = {arXiv},
  arxivid       = {arXiv:1802.08080v2},
  doi           = {10.1007/978-3-319-93000-8_95},
  eprint        = {arXiv:1802.08080v2},
  file          = {:home/alkhaldieid/Desktop/papers/papers/ICIAR papers/Classification of Breast Cancer Histology using
Deep Learning.pdf:pdf},
  isbn          = {9783319929996},
  keywords      = {Breast cancer,CNN,Deep learning,Histopathology,Transfer learning,majority voting},
  mendeley-tags = {majority voting},
}
@article{Wu2018a,
abstract = {Adjusting the learning rate schedule in stochastic gradient methods is an important unresolved problem which requires tuning in practice. If certain parameters of the loss function such as smoothness or strong convexity constants are known, theoretical learning rate schedules can be applied. However, in practice, such parameters are not known, and the loss function of interest is not convex in any case. The recently proposed batch normalization reparametrization is widely adopted in most neural network architectures today because, among other advantages, it is robust to the choice of Lipschitz constant of the gradient in loss function, allowing one to set a large learning rate without worry. Inspired by batch normalization, we propose a general nonlinear update rule for the learning rate in batch and stochastic gradient descent so that the learning rate can be initialized at a high value, and is subsequently decreased according to gradient observations along the way. The proposed method is shown to achieve robustness to the relationship between the learning rate and the Lipschitz constant, and near-optimal convergence rates in both the batch and stochastic settings ({\$}O(1/T){\$} for smooth loss in the batch setting, and {\$}O(1/\backslashsqrt{\{}T{\}}){\$} for convex loss in the stochastic setting). We also show through numerical evidence that such robustness of the proposed method extends to highly nonconvex and possibly non-smooth loss function in deep learning problems.Our analysis establishes some first theoretical understanding into the observed robustness for batch normalization and weight normalization.},
archivePrefix = {arXiv},
arxivId = {1803.02865},
author = {Wu, Xiaoxia and Ward, Rachel and Bottou, L{\'{e}}on},
eprint = {1803.02865},
file = {:home/alkhaldieid/Desktop/papers/papers/Learning Rate Schedueling/WNGrad{\_} Learn the Learning Rate in Gradient Descent.pdf:pdf;:home/alkhaldieid/Desktop/papers/papers/Learning Rate Schedueling/WNGrad{\_} Learn the Learning Rate in Gradient Descent{\_}backup.pdf:pdf},
pages = {1--16},
title = {{WNGrad: Learn the Learning Rate in Gradient Descent}},
url = {http://arxiv.org/abs/1803.02865},
year = {2018}
}

@Article{Bardou2018a,
  author   = {Bardou, Dalal and Zhang, Kun and Ahmad, Sayed Mohammad},
  title    = {{Classification of Breast Cancer Based on Histology Images Using Convolutional Neural Networks}},
  journal  = {IEEE Access},
  year     = {2018},
  volume   = {6},
  pages    = {24680--24693},
  issn     = {21693536},
  abstract = {Breast cancer is one of the main causes of cancer death worldwide. The diagnosis of biopsy tissue with hematoxylin and eosin stained images is non-trivial and specialists often disagree on the final diagnosis. Computer-aided Diagnosis systems contribute to reduce the cost and increase the efficiency of this process. Conventional classification approaches rely on feature extraction methods designed for a specific problem based on field-knowledge. To overcome the many difficulties of the feature-based approaches, deep learning methods are becoming important alternatives. A method for the classification of hematoxylin and eosin stained breast biopsy images using Convolutional Neural Networks (CNNs) is proposed. Images are classified in four classes, normal tissue, benign lesion, in situ carcinoma and invasive carcinoma, and in two classes, carcinoma and non-carcinoma. The architecture of the network is designed to retrieve information at different scales, including both nuclei and overall tissue organization. This design allows the extension of the proposed system to whole-slide histology images. The features extracted by the CNN are also used for training a Support Vector Machine classifier. Accuracies of 77.8{\%} for four class and 83.3{\%} for carcinoma/non-carcinoma are achieved. The sensitivity of our method for cancer cases is 95.6{\%}.},
  doi      = {10.1109/ACCESS.2018.2831280},
  file     = {:home/alkhaldieid/Desktop/papers/papers/ICIAR papers/Classification of breast cancer histology
images using Convolutional Neural Networks.pdf:pdf},
  isbn     = {1111111111},
  keywords = {Histology images,bag of words,convolutional neural networks,engineered features,locality constrained linear coding},
}
@article{Baldominos2018,
abstract = {Convolutional neural networks (CNNs) have been used over the past years to solve many different artificial intelligence (AI) problems, providing significant advances in some domains and leading to state-of-the-art results. However, the topologies of CNNs involve many different parameters, and in most cases, their design remains a manual process that involves effort and a significant amount of trial and error. In this work, we have explored the application of neuroevolution to the automatic design of CNN topologies, introducing a common framework for this task and developing two novel solutions based on genetic algorithms and grammatical evolution. We have evaluated our proposal using the MNIST dataset for handwritten digit recognition, achieving a result that is highly competitive with the state-of-the-art without any kind of data augmentation or preprocessing. When misclassified samples are carefully observed, it is found that most of them involve handwritten digits that are difficult to recognize even by a human.},
author = {Baldominos, Alejandro and Saez, Yago and Isasi, Pedro},
doi = {10.1016/j.neucom.2017.12.049},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Automatic topology design,Convolutional neural networks,Evolutionary algorithms,Genetic algorithms,Grammatical evolution,Neuroevolution},
month = {mar},
pages = {38--52},
publisher = {Elsevier B.V.},
title = {{Evolutionary convolutional neural networks: An application to handwriting recognition}},
volume = {283},
year = {2018}
}
@article{Komura2018,
abstract = {Abundant accumulation of digital histopathological images has led to the increased demand for their analysis, such as computer-aided diagnosis using machine learning techniques. However, digital pathological images and related tasks have some issues to be considered. In this mini-review, we introduce the application of digital pathological image analysis using machine learning algorithms, address some problems specific to such analysis, and propose possible solutions.},
author = {Komura, Daisuke and Ishikawa, Shumpei},
doi = {10.1016/j.csbj.2018.01.001},
file = {:home/alkhaldieid/Desktop/papers/papers/Ensemble/Machine Learning Methods for Histopathological Image Analysis.pdf:pdf;:home/alkhaldieid/Desktop/papers/papers/general histology papers/Machine Learning Methods for Histopathological Image Analysis.pdf:pdf},
issn = {20010370},
journal = {Computational and Structural Biotechnology Journal},
keywords = {Computer assisted diagnosis,Deep learning,Digital image analysis,Histopathology,Machine learning,Whole slide images},
pages = {34--42},
publisher = {The Authors},
title = {{Machine Learning Methods for Histopathological Image Analysis}},
url = {https://doi.org/10.1016/j.csbj.2018.01.001},
volume = {16},
year = {2018}
}
@article{Ioffe2015b,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82{\%} top-5 test error, exceeding the accuracy of human raters.},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
eprint = {1502.03167},
file = {:home/alkhaldieid/Desktop/papers/papers/BN/BNM.pdf:pdf},
isbn = {9781510810587},
journal = {32nd International Conference on Machine Learning, ICML 2015},
pages = {448--456},
title = {{Batch normalization: Accelerating deep network training by reducing internal covariate shift}},
volume = {1},
year = {2015}
}
@article{Flach2003,
abstract = {Many different metrics are used in machine learning and data mining to build and evaluate models. However, there is no general theory of machine learning metrics, that could answer questions such as: When we simultaneously want to optimise two criteria, how can or should they be traded off? Some metrics are inherently independent of class and misclassification cost distributions, while other are not - can this be made more precise? This paper provides a derivation of ROC space from first principles through 3D ROC space and the skew ratio, and redefines metrics in these dimensions. The paper demonstrates that the graphical depiction of machine learning metrics by means of ROC isometrics gives many useful insights into the characteristics of these metrics, and provides a foundation on which a theory of machine learning metrics can be built.},
author = {Flach, Peter A.},
file = {:home/alkhaldieid/Desktop/papers/papers/evalMetrics.pdf:pdf},
isbn = {1577351894},
journal = {Proceedings, Twentieth International Conference on Machine Learning},
pages = {194--201},
title = {{The Geometry of ROC Space: Understanding Machine Learning Metrics through ROC Isometrics}},
volume = {1},
year = {2003}
}
@article{Aresta2019a,
abstract = {Breast cancer is the most common invasive cancer in women, affecting more than 10{\%} of women worldwide. Microscopic analysis of a biopsy remains one of the most important methods to diagnose the type of breast cancer. This requires specialized analysis by pathologists, in a task that i) is highly time- and cost-consuming and ii) often leads to nonconsensual results. The relevance and potential of automatic classification algorithms using hematoxylin-eosin stained histopathological images has already been demonstrated, but the reported results are still sub-optimal for clinical use. With the goal of advancing the state-of-the-art in automatic classification, the Grand Challenge on BreAst Cancer Histology images (BACH) was organized in conjunction with the 15th International Conference on Image Analysis and Recognition (ICIAR 2018). BACH aimed at the classification and localization of clinically relevant histopathological classes in microscopy and whole-slide images from a large annotated dataset, specifically compiled and made publicly available for the challenge. Following a positive response from the scientific community, a total of 64 submissions, out of 677 registrations, effectively entered the competition. The submitted algorithms improved the state-of-the-art in automatic classification of breast cancer with microscopy images to an accuracy of 87{\%}. Convolutional neuronal networks were the most successful methodology in the BACH challenge. Detailed analysis of the collective results allowed the identification of remaining challenges in the field and recommendations for future developments. The BACH dataset remains publicly available as to promote further improvements to the field of automatic classification in digital pathology.},
archivePrefix = {arXiv},
arxivId = {1808.04277},
author = {Aresta, Guilherme and Ara{\'{u}}jo, Teresa and Kwok, Scotty and Chennamsetty, Sai Saketh and Safwan, Mohammed and Alex, Varghese and Marami, Bahram and Prastawa, Marcel and Chan, Monica and Donovan, Michael and Fernandez, Gerardo and Zeineh, Jack and Kohl, Matthias and Walz, Christoph and Ludwig, Florian and Braunewell, Stefan and Baust, Maximilian and Vu, Quoc Dang and To, Minh Nguyen Nhat and Kim, Eal and Kwak, Jin Tae and Galal, Sameh and Sanchez-Freire, Veronica and Brancati, Nadia and Frucci, Maria and Riccio, Daniel and Wang, Yaqi and Sun, Lingling and Ma, Kaiqiang and Fang, Jiannan and Kone, Ismael and Boulmane, Lahsen and Campilho, Aur{\'{e}}lio and Eloy, Catarina and Pol{\'{o}}nia, Ant{\'{o}}nio and Aguiar, Paulo},
doi = {10.1016/j.media.2019.05.010},
eprint = {1808.04277},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Breast cancer,Challenge,Comparative study,Deep learning,Digital pathology,Histology},
month = {aug},
pages = {122--139},
publisher = {Elsevier B.V.},
title = {{BACH: Grand challenge on breast cancer histology images}},
volume = {56},
year = {2019}
}
@book{Negnevitskya,
author = {Negnevitsky, Michael},
file = {:home/alkhaldieid/Desktop/papers/books/Artificial{\_}Intelligence-A{\_}Guide{\_}to{\_}Intelligent{\_}Systems.pdf:pdf},
isbn = {0321204662},
title = {{Artificial Intelligence}}
}
@article{Javeed2019,
author = {Javeed, Ashir and Zhou, Shijie and Yongjian, Liao and Qasim, Iqbal and Noor, Adeeb},
doi = {10.1109/ACCESS.2019.2952107},
file = {:home/alkhaldieid/Desktop/papers/papers/hyperparameter opt/08894128.pdf:pdf},
journal = {IEEE Access},
pages = {1},
publisher = {IEEE},
title = {{An Intelligent Learning System based on Random Search Algorithm and Optimized Random Forest Model for Improved Heart Disease Detection}},
volume = {PP},
year = {2019}
}
@book{Singh2018,
abstract = {Our understanding of the natural universe is far from being comprehensive. The following questions bring to the fore some of the fundamental issues. Is there a reality of information associated with the states of matter based entirely on natural causation? If so, then what constitutes the mechanism of information exchange (processing) at each interaction of physical entities? Let the association of information with a state of matter be referred to as the representation of semantic value expressed by the information. We ask, can the semantic value be quantified, described, and operated upon with symbols, as mathematical symbols describe the material world? In this work, these questions are dealt with substantively to establish the fundamental principles of the mechanisms of representation and propagation of information with every physical interaction. A quantitative method of information processing is derived from the first principles to show how high level structured and abstract semantics may arise via physical interactions alone, without a need for an intelligent interpreter. It is further shown that the natural representation constitutes a basis for the description, and therefore, for comprehension, of all natural phenomena, creating a more holistic view of nature. A brief discussion underscores the natural information processing as the foundation for the genesis of language and mathematics. In addition to the derivation of theoretical basis from established observations, the method of information processing is further demonstrated by a computer simulation.},
author = {Singh, Rajiv K.},
booktitle = {Information (Switzerland)},
doi = {10.3390/info9070168},
file = {:home/alkhaldieid/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Singh - 2018 - Fundamentals of natural representation.pdf:pdf},
isbn = {9781420011449},
issn = {20782489},
keywords = {Emergence of abstract semantics,Equivalence of physical interaction and informatio,Physics of representation,Reality of information in nature,Semantic value of information},
number = {7},
title = {{Fundamentals of natural representation}},
volume = {9},
year = {2018}
}
@article{TrEC,
abstract = {Documentation of commonly used measures in TREC evaluations},
author = {TrEC},
file = {:home/alkhaldieid/Desktop/papers/papers/CE.MEASURES06.pdf:pdf},
journal = {ReCALL},
title = {{trec eval Evaluation Report}}
}
@article{Williams2006,
abstract = {The identification of network applications through observation of associated packet traffic flows is vital to the areas of network management and surveillance. Currently popular methods such as port number and payload-based identification exhibit a number of shortfalls. An alternative is to use machine learning (ML) techniques and identify network applications based on per-flow statistics, derived from payload-independent features such as packet length and inter-arrival time distributions. The performance impact of feature set reduction, using Consistency-based and Correlation-based feature selection, is demonstrated on Na{\"{i}}ve Bayes, C4.5, Bayesian Network and Na{\"{i}}ve Bayes Tree algorithms. We then show that it is useful to differentiate algorithms based on computational performance rather than classification accuracy alone, as although classification accuracy between the algorithms is similar, computational performance can differ significantly.},
author = {Williams, Nigel and Zander, Sebastian and Armitage, Grenville},
doi = {10.1145/1163593.1163596},
file = {:home/alkhaldieid/Desktop/papers/papers/A{\_}preliminary{\_}performance{\_}comparison{\_}of{\_}five{\_}machi.pdf:pdf},
issn = {01464833},
journal = {Computer Communication Review},
keywords = {Machine learning,Traffic classification},
number = {5},
pages = {7--15},
title = {{A preliminary performance comparison of five machine learning algorithms for practical IP traffic flow classification}},
volume = {36},
year = {2006}
}
@article{Long2015,
abstract = {Recent studies reveal that a deep neural network can learn transferable features which generalize well to novel tasks for domain adaptation. However, as deep features eventually transition from general to specific along the network, the feature transferability drops significantly in higher layers with increasing domain discrepancy. Hence, it is important to formally reduce the dataset bias and enhance the transferability in task-specific layers. In this paper, we propose a new Deep Adaptation Network (DAN) architecture, which generalizes deep convolutional neural network to the domain adaptation scenario. In DAN, hidden representations of all task-specific layers are embedded in a reproducing kernel Hilbert space where the mean embeddings of different domain distributions can be explicitly matched. The domain discrepancy is further reduced using an optimal multi-kernel selection method for mean embedding matching. DAN can learn transferable features with statistical guarantees, and can scale linearly by unbiased estimate of kernel embedding. Extensive empirical evidence shows that the proposed architecture yields state-of-the-art image classification error rates on standard domain adaptation benchmarks.},
archivePrefix = {arXiv},
arxivId = {1502.02791},
author = {Long, Mingsheng and Cao, Yue and Wang, Jianmin and Jordan, Michael I.},
eprint = {1502.02791},
file = {:home/alkhaldieid/Desktop/papers/papers/Learning Rate Schedueling/Learning Transferable Features with Deep Adaptation Networks.pdf:pdf},
isbn = {9781510810587},
journal = {32nd International Conference on Machine Learning, ICML 2015},
keywords = {Deep learning,domain adaptation,two-sample test},
pages = {97--105},
title = {{Learning transferable features with deep adaptation networks}},
volume = {1},
year = {2015}
}
@article{Smith2017a,
abstract = {It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally find the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of fixed values achieves improved classification accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate 'reasonable bounds' - linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10 and CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks.},
archivePrefix = {arXiv},
arxivId = {1506.01186},
author = {Smith, Leslie N.},
doi = {10.1109/WACV.2017.58},
eprint = {1506.01186},
file = {:home/alkhaldieid/Desktop/papers/papers/Learning Rate Schedueling/Cyclical Learning Rates for Training Neural Networks.pdf:pdf},
isbn = {9781509048229},
journal = {Proceedings - 2017 IEEE Winter Conference on Applications of Computer Vision, WACV 2017},
number = {April},
pages = {464--472},
title = {{Cyclical learning rates for training neural networks}},
year = {2017}
}
@article{Alenezi1934,
author = {Alenezi, Fayadh and Salari, Ezzatollah},
file = {:home/alkhaldieid/Desktop/papers/Samples/Perceptual-Local-Contrast-Enhancement-and-Global-Variance-Minimization-of-Medical-Images-for-Improved-Fusion.pdf:pdf},
number = {3},
pages = {1--8},
title = {{Perceptual Local Contrast Enhancement and Global Variance Minimization of Medical Images for Improved Fusion}},
year = {1934}
}
@article{Aresta2019,
abstract = {Breast cancer is the most common invasive cancer in women, affecting more than 10{\%} of women worldwide. Microscopic analysis of a biopsy remains one of the most important methods to diagnose the type of breast cancer. This requires specialized analysis by pathologists, in a task that i) is highly time- and cost-consuming and ii) often leads to nonconsensual results. The relevance and potential of automatic classification algorithms using hematoxylin-eosin stained histopathological images has already been demonstrated, but the reported results are still sub-optimal for clinical use. With the goal of advancing the state-of-the-art in automatic classification, the Grand Challenge on BreAst Cancer Histology images (BACH) was organized in conjunction with the 15th International Conference on Image Analysis and Recognition (ICIAR 2018). BACH aimed at the classification and localization of clinically relevant histopathological classes in microscopy and whole-slide images from a large annotated dataset, specifically compiled and made publicly available for the challenge. Following a positive response from the scientific community, a total of 64 submissions, out of 677 registrations, effectively entered the competition. The submitted algorithms improved the state-of-the-art in automatic classification of breast cancer with microscopy images to an accuracy of 87{\%}. Convolutional neuronal networks were the most successful methodology in the BACH challenge. Detailed analysis of the collective results allowed the identification of remaining challenges in the field and recommendations for future developments. The BACH dataset remains publicly available as to promote further improvements to the field of automatic classification in digital pathology.},
archivePrefix = {arXiv},
arxivId = {arXiv:1808.04277v1},
author = {Aresta, Guilherme and Ara{\'{u}}jo, Teresa and Kwok, Scotty and Chennamsetty, Sai Saketh and Safwan, Mohammed and Alex, Varghese and Marami, Bahram and Prastawa, Marcel and Chan, Monica and Donovan, Michael and Fernandez, Gerardo and Zeineh, Jack and Kohl, Matthias and Walz, Christoph and Ludwig, Florian and Braunewell, Stefan and Baust, Maximilian and Vu, Quoc Dang and To, Minh Nguyen Nhat and Kim, Eal and Kwak, Jin Tae and Galal, Sameh and Sanchez-Freire, Veronica and Brancati, Nadia and Frucci, Maria and Riccio, Daniel and Wang, Yaqi and Sun, Lingling and Ma, Kaiqiang and Fang, Jiannan and Kone, Ismael and Boulmane, Lahsen and Campilho, Aur{\'{e}}lio and Eloy, Catarina and Pol{\'{o}}nia, Ant{\'{o}}nio and Aguiar, Paulo},
doi = {10.1016/j.media.2019.05.010},
eprint = {arXiv:1808.04277v1},
file = {:home/alkhaldieid/Desktop/papers/papers/dataset/BACH$\backslash$: Grand Challenge on Breast Cancer Histology Images:},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Breast cancer,Challenge,Comparative study,Deep learning,Digital pathology,Histology},
pages = {122--139},
title = {{BACH: Grand challenge on breast cancer histology images}},
volume = {56},
year = {2019}
}
@article{Ioffe2015a,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9{\%} top-5 validation error (and 4.8{\%} test error), exceeding the accuracy of human raters.},
archivePrefix = {arXiv},
arxivId = {arXiv:1502.03167v3},
author = {Ioffe, Sergey and Szegedy, Christian},
eprint = {arXiv:1502.03167v3},
file = {:home/alkhaldieid/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ioffe, Szegedy - 2015 - Batch normalization Accelerating deep network training by reducing internal covariate shift.pdf:pdf},
isbn = {9781510810587},
journal = {32nd International Conference on Machine Learning, ICML 2015},
pages = {448--456},
title = {{Batch normalization: Accelerating deep network training by reducing internal covariate shift}},
volume = {1},
year = {2015}
}
@article{Alenezi2018,
author = {Alenezi, Fayadh and Salari, Ezzatollah},
file = {:home/alkhaldieid/Desktop/papers/Samples/13-fayadh-alenezi.pdf:pdf},
keywords = {gabor filter,imfilter,imfuse,maximum selection,medical image fusion,pcnn},
pages = {72--81},
title = {{A Novel Pulse-Coupled Neural Network using Gabor Filters for Medical Image Fusion}},
volume = {8491},
year = {2018}
}
@article{Alenezi2019,
author = {Alenezi, Fayadh and Salari, Ezzatollah},
file = {:home/alkhaldieid/Desktop/papers/Samples/7-fayadh-alenezi.pdf:pdf},
keywords = {bilateral filter,block toeplitz matrix,contrast adjustment,equalization,gaussian kernel,histogram,medical image},
pages = {37--42},
title = {{A Novel Block Toeplitz Matrix for DCT-based , Perceptually Enhanced Image Fusion}},
volume = {8491},
year = {2019}
}
@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different “thinned” networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
file = {:home/alkhaldieid/Desktop/papers/papers/regulizations/Dropout.pdf:pdf},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Deep learning,Model combination,Neural networks,Regularization},
pages = {1929--1958},
title = {{Dropout: A simple way to prevent neural networks from overfitting}},
volume = {15},
year = {2014}
}

@Article{Kingma2014a,
  author        = {Kingma, Diederik P. and Ba, Jimmy},
  title         = {{Adam: A Method for Stochastic Optimization}},
  year          = {2014},
  pages         = {1--15},
  abstract      = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  arxivid       = {1412.6980},
  eprint        = {1412.6980},
  file          = {:home/alkhaldieid/Desktop/papers/papers/optimizers/adamax.pdf:pdf},
  url           = {http://arxiv.org/abs/1412.6980},
}
@article{Zeiler2012,
abstract = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
archivePrefix = {arXiv},
arxivId = {1212.5701},
author = {Zeiler, Matthew D.},
eprint = {1212.5701},
file = {:home/alkhaldieid/Desktop/papers/papers/optimizers/ADADELTA$\backslash$: AN ADAPTIVE LEARNING RATE METHOD:},
title = {{ADADELTA: An Adaptive Learning Rate Method}},
url = {http://arxiv.org/abs/1212.5701},
year = {2012}
}
@article{Lecun1998,
abstract = {We present a fast, fully parameterizable GPU implementation of Convolutional Neural Network variants. Our feature extractors are neither carefully designed nor pre-wired, but rather learned in a supervised way. Our deep hierarchical architectures achieve the best published results on benchmarks for object classification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with error rates of 2.53{\%}, 19.51{\%}, 0.35{\%}, respectively. Deep nets trained by simple back-propagation perform better than more shallow ones. Learning is surprisingly rapid. NORB is completely trained within five epochs. Test error rates on MNIST drop to 2.42{\%}, 0.97{\%} and 0.48{\%} after 1, 3 and 17 epochs, respectively.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Lecun, Yann and Bottou, Leon and Bengio, Yoshua and Ha, Patrick},
doi = {10.1109/5.726791},
eprint = {1102.0183},
file = {:home/alkhaldieid/Desktop/papers/papers/Lecun98.pdf:pdf},
isbn = {0018-9219},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {convo-,document recogni-,fi-,gradient-based learning,graph transformer networks,lutional neural networks,machine learning,neural networks,nite state transducers,ocr,tion},
number = {November},
pages = {1--46},
pmid = {15823584},
title = {{LeNet}},
year = {1998}
}
@article{,
file = {:home/alkhaldieid/Desktop/papers/papers/lecun-99.pdf:pdf},
number = {0},
title = {{ج. روستایی،م. فاضلی،ع.ا. رشیديمهرآبادي، استفاده از نانوفیلتراسیون در تصفیه آبهاي لبشور ( مطالعه موردي چاههاي کهورستانNo Title}}
}
@article{Sun2019,
abstract = {Evolutionary computation methods have been successfully applied to neural networks since two decades ago, while those methods cannot scale well to the modern deep neural networks due to the complicated architectures and large quantities of connection weights. In this paper, we propose a new method using genetic algorithms for evolving the architectures and connection weight initialization values of a deep convolutional neural network to address image classification problems. In the proposed algorithm, an efficient variable-length gene encoding strategy is designed to represent the different building blocks and the unpredictable optimal depth in convolutional neural networks. In addition, a new representation scheme is developed for effectively initializing connection weights of deep convolutional neural networks, which is expected to avoid networks getting stuck into local minima which is typically a major issue in the backward gradient-based optimization. Furthermore, a novel fitness evaluation method is proposed to speed up the heuristic search with substantially less computational resource. The proposed algorithm is examined and compared with 22 existing algorithms on nine widely used image classification tasks, including the state-of-the-art methods. The experimental results demonstrate the remarkable superiority of the proposed algorithm over the state-of-the-art algorithms in terms of classification error rate and the number of parameters (weights).},
archivePrefix = {arXiv},
arxivId = {1710.10741},
author = {Sun, Yanan and Xue, Bing and Zhang, Mengjie and Yen, Gary G.},
doi = {10.1109/tevc.2019.2916183},
eprint = {1710.10741},
file = {:home/alkhaldieid/Desktop/papers/papers/hyperparameter opt/1710.10741.pdf:pdf},
issn = {1089-778X},
journal = {IEEE Transactions on Evolutionary Computation},
pages = {1--1},
title = {{Evolving Deep Convolutional Neural Networks for Image Classification}},
year = {2019}
}
@article{Alom2019,
abstract = {Deep Learning (DL) approaches have been providing state-of-the-art performance in different modalities in the field of medical imagining including Digital Pathology Image Analysis (DPIA). Out of many different DL approaches, Deep Convolutional Neural Network (DCNN) technique provides superior performance for classification, segmentation, and detection tasks. Most of the task in DPIA problems are somehow possible to solve with classification, segmentation, and detection approaches. In addition, sometimes pre and post-processing methods are applied for solving some specific type of problems. Recently, different DCNN models including Inception residual recurrent CNN (IRRCNN), Densely Connected Recurrent Convolution Network (DCRCN), Recurrent Residual U-Net (R2U-Net), and R2U-Net based regression model (UD-Net) have proposed and provide state-of-the-art performance for different computer vision and medical image analysis tasks. However, these advanced DCNN models have not been explored for solving different problems related to DPIA. In this study, we have applied these DCNN techniques for solving different DPIA problems and evaluated on different publicly available benchmark datasets for seven different tasks in digital pathology including lymphoma classification, Invasive Ductal Carcinoma (IDC) detection, nuclei segmentation, epithelium segmentation, tubule segmentation, lymphocyte detection, and mitosis detection. The experimental results are evaluated with different performance metrics such as sensitivity, specificity, accuracy, F1-score, Receiver Operating Characteristics (ROC) curve, dice coefficient (DC), and Means Squired Errors (MSE). The results demonstrate superior performance for classification, segmentation, and detection tasks compared to existing machine learning and DCNN based approaches.},
archivePrefix = {arXiv},
arxivId = {1904.09075},
author = {Alom, Md Zahangir and Aspiras, Theus and Taha, Tarek M. and Asari, Vijayan K. and Bowen, TJ and Billiter, Dave and Arkell, Simon},
eprint = {1904.09075},
file = {:home/alkhaldieid/Desktop/papers/papers/future/Advanced Deep Convolutional Neural Network Approaches for.pdf:pdf},
keywords = {and ud-net,computational pathology,digital pathology,drcn,irrcnn,r2u-net},
title = {{Advanced Deep Convolutional Neural Network Approaches for Digital Pathology Image Analysis: a comprehensive evaluation with different use cases}},
url = {http://arxiv.org/abs/1904.09075},
volume = {2},
year = {2019}
}
@article{Hendrycks2019,
abstract = {He et al. (2018) have called into question the utility of pre-training by showing that training from scratch can often yield similar performance to pre-training. We show that although pre-training may not improve performance on traditional classification metrics, it improves model robustness and uncertainty estimates. Through extensive experiments on adversarial examples, label corruption, class imbalance, out-of-distribution detection, and confidence calibration, we demonstrate large gains from pre-training and complementary effects with task-specific methods. We introduce adversarial pre-training and show approximately a 10{\%} absolute improvement over the previous state-of-the-art in adversarial robustness. In some cases, using pre-training without task-specific methods also surpasses the state-of-the-art, highlighting the need for pre-training when evaluating future methods on robustness and uncertainty tasks.},
archivePrefix = {arXiv},
arxivId = {1901.09960},
author = {Hendrycks, Dan and Lee, Kimin and Mazeika, Mantas},
eprint = {1901.09960},
file = {:home/alkhaldieid/Desktop/papers/papers/transfer learning papers/Using Pre-Training Can Improve Model Robustness and Uncertainty.pdf:pdf},
number = {2018},
title = {{Using Pre-Training Can Improve Model Robustness and Uncertainty}},
url = {http://arxiv.org/abs/1901.09960},
year = {2019}
}
@article{Tabik2017b,
abstract = {{\textcopyright} 2017, the Authors. In the last five years, deep learning methods and particularly Convolutional Neural Networks (CNNs) have exhibited excellent accuracies in many pattern classification problems. Most of the state-of-the-art models apply data-augmentation techniques at the training stage. This paper provides a brief tutorial on data preprocessing and shows its benefits by using the competitive MNIST handwritten digits classification problem. We show and analyze the impact of different preprocessing techniques on the performance of three CNNs, LeNet, Network3 and DropConnect, together with their ensembles. The analyzed transformations are, centering, elastic deformation, translation, rotation and different combinations of them. Our analysis demonstrates that data-preprocessing techniques, such as the combination of elastic deformation and rotation, together with ensembles have a high potential to further improve the state-of-the-art accuracy in MNIST classification.},
author = {Tabik, Siham and Peralta, Daniel and Herrera-Poyatos, Andr{\'{e}}s and Herrera, Francisco},
doi = {10.2991/ijcis.2017.10.1.38},
file = {:home/alkhaldieid/Desktop/papers/papers/Preprossing/snapshot of preproc in cnn mnist.pdf:pdf;:home/alkhaldieid/Desktop/papers/papers/Preprossing/snapshot of preproc in cnn mnist{\_}backup.pdf:pdf},
issn = {18756883},
journal = {International Journal of Computational Intelligence Systems},
keywords = {Classification,Convolutional Neural Networks (CNNs),Data augmentation,Deep learning,Handwritten digits,Preprocessing},
number = {1},
pages = {555--568},
title = {{A snapshot of image Pre-Processing for convolutional neural networks: Case study of MNIST}},
volume = {10},
year = {2017}
}

@Article{Yi2019a,
  author        = {Yi, Xin and Walia, Ekta and Babyn, Paul},
  title         = {{Generative adversarial network in medical imaging: A review}},
  journal       = {Medical Image Analysis},
  year          = {2019},
  volume        = {58},
  pages         = {101552},
  issn          = {13618415},
  abstract      = {Generative adversarial networks have gained a lot of attention in the computer vision community due to their capability of data generation without explicitly modelling the probability density function. The adversarial loss brought by the discriminator provides a clever way of incorporating unlabeled samples into training and imposing higher order consistency. This has proven to be useful in many cases, such as domain adaptation, data augmentation, and image-to-image translation. These properties have attracted researchers in the medical imaging community, and we have seen rapid adoption in many traditional and novel applications, such as image reconstruction, segmentation, detection, classification, and cross-modality synthesis. Based on our observations, this trend will continue and we therefore conducted a review of recent advances in medical imaging using the adversarial training scheme with the hope of benefiting researchers interested in this technique.},
  archiveprefix = {arXiv},
  arxivid       = {arXiv:1809.07294v2},
  doi           = {10.1016/j.media.2019.101552},
  eprint        = {arXiv:1809.07294v2},
  file          = {:home/alkhaldieid/Desktop/papers/papers/gan/Generative Adversarial Network in Medical Imaging$\backslash$: A Review:},
  keywords      = {deep learning,generative adversarial network,generative model,medical imaging,review},
}
@article{Shi,
abstract = {Aiming to produce sufficient and diverse training samples, data augmentation has been demonstrated for its effectiveness in training deep models. Regarding that the criterion of the best augmentation is challenging to define, we in this paper present a novel learning-based augmentation method termed as DeepAugNet, which formulates the final augmented data as a collection of several sequentially augmented subsets. Specifically, the current augmented subset is required to maximize the performance improvement compared with the last augmented subset by learning the deterministic augmentation policy using deep reinforcement learning. By introducing an unified optimization goal, DeepAugNet intends to combine the data augmentation and the deep model training in an end-to-end training manner which is realized by simultaneously training a hybrid architecture of dueling deep Q-learning algorithm and a surrogate deep model. We extensively evaluated our proposed DeepAugNet on various benchmark datasets including Fashion MNIST, CUB, CIFAR-100 and WebCaricature. Compared with the current state-of-the-arts, our method can achieve a significant improvement in small-scale datasets, and a comparable performance in large-scale datasets. Code will be available soon.},
archivePrefix = {arXiv},
arxivId = {1910.08343},
author = {Shi, Yinghuan and Qin, Tiexin and Liu, Yong and Lu, Jiwen and Gao, Yang and Shen, Dinggang},
eprint = {1910.08343},
file = {:home/alkhaldieid/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shi et al. - Unknown - Automatic Data Augmentation by Learning the Deterministic Policy.pdf:pdf},
title = {{Automatic Data Augmentation by Learning the Deterministic Policy}},
url = {http://arxiv.org/abs/1910.08343},
year = {2019}
}
@article{Perez2017,
abstract = {In this paper, we explore and compare multiple solutions to the problem of data augmentation in image classification. Previous work has demonstrated the effectiveness of data augmentation through simple techniques, such as cropping, rotating, and flipping input images. We artificially constrain our access to data to a small subset of the ImageNet dataset, and compare each data augmentation technique in turn. One of the more successful data augmentations strategies is the traditional transformations mentioned above. We also experiment with GANs to generate images of different styles. Finally, we propose a method to allow a neural net to learn augmentations that best improve the classifier, which we call neural augmentation. We discuss the successes and shortcomings of this method on various datasets.},
archivePrefix = {arXiv},
arxivId = {1712.04621},
author = {Perez, Luis and Wang, Jason},
eprint = {1712.04621},
file = {:home/alkhaldieid/Desktop/papers/papers/data aug/The Effectiveness of Data Augmentation in Image Classification using Deep:},
title = {{The Effectiveness of Data Augmentation in Image Classification using Deep Learning}},
url = {http://arxiv.org/abs/1712.04621},
year = {2017}
}
@article{AzevedoTosta2017,
abstract = {Image processing techniques are being widely developed for helping specialists in analysis of histological images obtained from biopsies for diagnoses and prognoses determination. Several types of cancer can be diagnosed using segmentation methods that are capable to identify specific neoplastic regions. The use of these computational methods makes the analysis of experts more objective and less time-consuming. Thus, the progressive development of histological images segmentation is an important step for modern medicine. This study presents the progress of recent advances in methods for segmentation of chronic lymphocytic leukemia, follicular lymphoma and mantle cell lymphoma images. The paper shows the main techniques of image processing employed in the stages of preprocessing, detection/segmentation and post-processing of published approaches and discusses their advantages and disadvantages. This study presents the most often used segmentation techniques for these images segmentation, such as thresholding, region-based methods and K-means clustering algorithm. The addressed cancers are also described in histological details as well as possible variations in the tissue preparation and its digitization. Besides, it includes a review of validation techniques and discusses the potential future directions of research in the segmentation of these neoplasias.},
author = {{Azevedo Tosta}, Tha{\'{i}}na A. and Neves, Leandro A. and do Nascimento, Marcelo Z.},
doi = {10.1016/j.imu.2017.05.009},
file = {:home/alkhaldieid/Desktop/papers/papers/future/Segmentation methods of H{\&}E-stained histological images of lymphoma A review.pdf:pdf},
issn = {23529148},
journal = {Informatics in Medicine Unlocked},
keywords = {Chronic lymphocytic leukemia,Follicular lymphoma,Histological images,Mantle cell lymphoma,Segmentation,clinical importance of early diagnosis,preprocessig},
mendeley-tags = {clinical importance of early diagnosis,preprocessig},
number = {February},
pages = {35--43},
publisher = {Elsevier Ltd},
title = {{Segmentation methods of H{\&}E-stained histological images of lymphoma: A review}},
url = {https://doi.org/10.1016/j.imu.2017.05.009},
volume = {9},
year = {2017}
}
@article{Ali2019,
author = {Ali, Liaqat and Zhu, Ce and Zhang, Zhonghao and Liu, Yipeng},
doi = {10.1109/jtehm.2019.2940900},
file = {:home/alkhaldieid/Desktop/papers/papers/hyperparameter opt/08861144.pdf:pdf},
journal = {IEEE Journal of Translational Engineering in Health and Medicine},
pages = {1--1},
publisher = {IEEE},
title = {{Automated Detection of Parkinson's Disease Based on Multiple Types of Sustained Phonations using Linear Discriminant Analysis and Genetically Optimized Neural Network}},
volume = {PP},
year = {2019}
}
@article{Levesque2016,
abstract = {In this paper, we bridge the gap between hyperparameter optimization and ensemble learning by performing Bayesian optimization of an ensemble with regards to its hyperparameters. Our method consists in building a fixed-size ensemble, optimizing the configuration of one classifier of the ensemble at each iteration of the hyperparameter optimization algorithm, taking into consideration the interaction with the other models when evaluating potential performances. We also consider the case where the ensemble is to be reconstructed at the end of the hyperparameter optimization phase, through a greedy selection over the pool of models generated during the optimization. We study the performance of our proposed method on three different hyperparameter spaces, showing that our approach is better than both the best single model and a greedy ensemble construction over the models produced by a standard Bayesian optimization.},
archivePrefix = {arXiv},
arxivId = {1605.06394},
author = {L{\'{e}}vesque, Julien Charles and Gagn{\'{e}}, Christian and Sabourin, Robert},
eprint = {1605.06394},
file = {:home/alkhaldieid/Desktop/papers/papers/hyperparameter opt/1605.06394.pdf:pdf},
isbn = {9781510827806},
journal = {32nd Conference on Uncertainty in Artificial Intelligence 2016, UAI 2016},
pages = {437--446},
title = {{Bayesian hyperparameter optimization for ensemble learning}},
year = {2016}
}
@article{Abdikenov2019,
abstract = {Breast cancer prognostic modeling is difficult since it is governed by many diverse factors. Given the low median survival and large scale breast cancer data, which comes from high throughput technology, the accurate and reliable prognosis of breast cancer is becoming increasingly difficult. While accurate and timely prognosis may save many patients from going through painful and expensive treatments, it may also help oncologists in managing the disease more efficiently and effectively. Data analytics augmented by machine-learning algorithms have been proposed in past for breast cancer prognosis; and however, most of these could not perform well owing to the heterogeneous nature of available data and model interpretability related issues. A robust prognostic modeling approach is proposed here whereby a Pareto optimal set of deep neural networks (DNNs) exhibiting equally good performance metrics is obtained. The set of DNNs is initialized and their hyperparameters are optimized using the evolutionary algorithm, NSGAIII. The final DNN model is selected from the Pareto optimal set of many DNNs using a fuzzy inferencing approach. Contrary to using DNNs as the black box, the proposed scheme allows understanding how various performance metrics (such as accuracy, sensitivity, F1, and so on) change with changes in hyper-parameters. This enhanced interpretability can be further used to improve or modify the behavior of DNNs. The heterogeneous breast cancer database requires preprocessing for better interpretation of categorical variables in order to improve prognosis from classifiers. Furthermore, we propose to use a neural network-based entity-embedding method for categorical features with high cardinality. This approach can provide a vector representation of categorical features in multidimensional space with enhanced interpretability. It is shown with evidence that DNNs optimized using evolutionary algorithms exhibit improved performance over other classifiers mentioned in this paper.},
author = {Abdikenov, Beibit and Iklassov, Zangir and Sharipov, Askhat and Hussain, Shahid and Jamwal, Prashant K.},
doi = {10.1109/ACCESS.2019.2897078},
file = {:home/alkhaldieid/Desktop/papers/papers/hyperparameter opt/08632897.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Breast cancer prognostic modelling,deep learning networks,entity embedding,evolutionary algorithms,fuzzy inferencing},
pages = {18050--18060},
title = {{Analytics of Heterogeneous Breast Cancer Data Using Neuroevolution}},
volume = {7},
year = {2019}
}
@article{Correa2011,
abstract = {Background: The rapid identification of Bacillus spores and bacterial identification are paramount because of their implications in food poisoning, pathogenesis and their use as potential biowarfare agents. Many automated analytical techniques such as Curie-point pyrolysis mass spectrometry (Py-MS) have been used to identify bacterial spores giving use to large amounts of analytical data. This high number of features makes interpretation of the data extremely difficult We analysed Py-MS data from 36 different strains of aerobic endospore-forming bacteria encompassing seven different species. These bacteria were grown axenically on nutrient agar and vegetative biomass and spores were analyzed by Curie-point Py-MS.Results: We develop a novel genetic algorithm-Bayesian network algorithm that accurately identifies sand selects a small subset of key relevant mass spectra (biomarkers) to be further analysed. Once identified, this subset of relevant biomarkers was then used to identify Bacillus spores successfully and to identify Bacillus species via a Bayesian network model specifically built for this reduced set of features.Conclusions: This final compact Bayesian network classification model is parsimonious, computationally fast to run and its graphical visualization allows easy interpretation of the probabilistic relationships among selected biomarkers. In addition, we compare the features selected by the genetic algorithm-Bayesian network approach with the features selected by partial least squares-discriminant analysis (PLS-DA). The classification accuracy results show that the set of features selected by the GA-BN is far superior to PLS-DA. {\textcopyright} 2011 Correa and Goodacre; licensee BioMed Central Ltd.},
author = {Correa, Elon and Goodacre, Royston},
doi = {10.1186/1471-2105-12-33},
file = {:home/alkhaldieid/Desktop/papers/papers/hyperparameter opt/genetic algorithm-Bayesian network.pdf:pdf},
issn = {14712105},
journal = {BMC Bioinformatics},
number = {1},
pages = {33},
publisher = {BioMed Central Ltd},
title = {{A genetic algorithm-Bayesian network approach for the analysis of metabolomics and spectroscopic data: Application to the rapid identification of Bacillus spores and classification of Bacillus species}},
url = {http://www.biomedcentral.com/1471-2105/12/33},
volume = {12},
year = {2011}
}
@article{Anonymous2019,
author = {Anonymous},
file = {:home/alkhaldieid/Desktop/papers/papers/Learning Rate Schedueling/rethinking{\_}learning{\_}rate{\_}schedules{\_}for{\_}stochastic{\_}optimization.pdf:pdf},
pages = {1--19},
title = {{Rethinking Learning Rate Schedules for Stochastic Optimization}},
year = {2019}
}
@article{Bruna,
author = {Bruna, Joan},
file = {:home/alkhaldieid/Desktop/papers/papers/Joan{\_}Bruna{\_}external{\_}seminar{\_}21{\_}03{\_}2016{\_}abstract.pdf:pdf},
pages = {2016},
title = {{Convolutional Neural Networks against the curse of}}
}
@article{Thompson2019,
archivePrefix = {arXiv},
arxivId = {1912.02260},
author = {Thompson, Jessica and Bengio, Yoshua and Schoenwiesner, Marc},
doi = {10.32470/ccn.2019.1300-0},
eprint = {1912.02260},
file = {:home/alkhaldieid/Desktop/papers/papers/Learning Rate Schedueling/1912.02260.pdf:pdf},
keywords = {cca will,cnns,find the linear combinations,freeze,of variables x and,of x and y,random features,rv coefficient,similarity analysis,training,when comparing two sets,which maximizes their,y},
title = {{The effect of task and training on intermediate representations in convolutional neural networks revealed with modified RV similarity analysis}},
year = {2019}
}
@article{Alain2016,
abstract = {Neural network models have a reputation for being black boxes. We propose to monitor the features at every layer of a model and measure how suitable they are for classification. We use linear classifiers, which we refer to as "probes", trained entirely independently of the model itself. This helps us better understand the roles and dynamics of the intermediate layers. We demonstrate how this can be used to develop a better intuition about models and to diagnose potential problems. We apply this technique to the popular models Inception v3 and Resnet-50. Among other things, we observe experimentally that the linear separability of features increase monotonically along the depth of the model.},
archivePrefix = {arXiv},
arxivId = {1610.01644},
author = {Alain, Guillaume and Bengio, Yoshua},
eprint = {1610.01644},
file = {:home/alkhaldieid/Desktop/papers/papers/Learning Rate Schedueling/Understanding intermediate layers.pdf:pdf},
title = {{Understanding intermediate layers using linear classifier probes}},
url = {http://arxiv.org/abs/1610.01644},
year = {2016}
}
@article{Scherer2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1511.04747v6},
author = {Scherer, Stefan},
eprint = {arXiv:1511.04747v6},
file = {:home/alkhaldieid/Desktop/papers/papers/Learning Rate Schedueling/an{\_}exponential{\_}learning{\_}rate{\_}schedule{\_}for{\_}batch{\_}normalized{\_}networks.pdf:pdf},
pages = {1--10},
title = {{L Earning R Epresentations}},
volume = {1},
year = {2016}
}
@article{Yedida2019,
abstract = {Optimizing deep neural networks is largely thought to be an empirical process, requiring manual tuning of several hyper-parameters, such as learning rate, weight decay, and dropout rate. Arguably, the learning rate is the most important of these to tune, and this has gained more attention in recent works. In this paper, we propose a novel method to compute the learning rate for training deep neural networks with stochastic gradient descent. We first derive a theoretical framework to compute learning rates dynamically based on the Lipschitz constant of the loss function. We then extend this framework to other commonly used optimization algorithms, such as gradient descent with momentum and Adam. We run an extensive set of experiments that demonstrate the efficacy of our approach on popular architectures and datasets, and show that commonly used learning rates are an order of magnitude smaller than the ideal value.},
archivePrefix = {arXiv},
arxivId = {1902.07399},
author = {Yedida, Rahul and Saha, Snehanshu},
eprint = {1902.07399},
file = {:home/alkhaldieid/Desktop/papers/papers/Learning Rate Schedueling/A novel adaptive learning rate scheduler for deep.pdf:pdf},
pages = {1--21},
title = {{A novel adaptive learning rate scheduler for deep neural networks}},
url = {http://arxiv.org/abs/1902.07399},
year = {2019}
}
@article{Sun2019b,
abstract = {Meta-learning has been proposed as a framework to address the challenging few-shot learning setting. The key idea is to leverage a large number of similar few-shot tasks in order to learn how to adapt a base-learner to a new task for which only a few labeled samples are available. As deep neural networks (DNNs) tend to overfit using a few samples only, typical meta-learning models use shallow neural networks, thus limiting its effectiveness. In order to achieve top performance, some recent works tried to use the DNNs pre-trained on large-scale datasets but mostly in straight-forward manners, e.g., (1) taking their weights as a warm start of meta-training, and (2) freezing their convolutional layers as the feature extractor of base-learners. In this paper, we propose a novel approach called meta-transfer learning (MTL) which learns to transfer the weights of a deep NN for few-shot learning tasks. Specifically, meta refers to training multiple tasks, and transfer is achieved by learning scaling and shifting functions of DNN weights for each task. In addition, we introduce the hard task (HT) meta-batch scheme as an effective learning curriculum that further boosts the learning efficiency of MTL. We conduct few-shot learning experiments and report top performance for five-class few-shot recognition tasks on three challenging benchmarks: miniImageNet, tieredImageNet and Fewshot-CIFAR100 (FC100). Extensive comparisons to related works validate that our MTL approach trained with the proposed HT meta-batch scheme achieves top performance. An ablation study also shows that both components contribute to fast convergence and high accuracy.},
archivePrefix = {arXiv},
arxivId = {1910.03648},
author = {Sun, Qianru and Liu, Yaoyao and Chen, Zhaozheng and Chua, Tat-Seng and Schiele, Bernt},
eprint = {1910.03648},
file = {:home/alkhaldieid/Desktop/papers/papers/Learning Rate Schedueling/Meta-Transfer Learning through Hard Tasks.pdf:pdf},
pages = {1--14},
title = {{Meta-Transfer Learning through Hard Tasks}},
url = {http://arxiv.org/abs/1910.03648},
year = {2019}
}
@article{Houlsby2019,
abstract = {Fine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter's effectiveness, we transfer the recently proposed BERT Transformer model to 26 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.4{\%} of the performance of full fine-tuning, adding only 3.6{\%} parameters per task. By contrast, fine-tuning trains 100{\%} of the parameters per task.},
archivePrefix = {arXiv},
arxivId = {1902.00751},
author = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and de Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
eprint = {1902.00751},
file = {:home/alkhaldieid/Desktop/papers/papers/Learning Rate Schedueling/Parameter-Efficient Transfer Learning for NLP.pdf:pdf},
title = {{Parameter-Efficient Transfer Learning for NLP}},
url = {http://arxiv.org/abs/1902.00751},
year = {2019}
}
@article{Iranmanesh2009,
abstract = {In this paper a high speed learning method using differential adaptive learning rate (DALRM) is proposed. Comparison of this method with other methods such as standard BP, Nguyen-Widrow weight Initialization and Optical BP shows that the network's learning speed has highly increased. Learning often takes a long time to converge and it may fall into local minimas. One way of escaping from local minima is to use a large learning rate at first and then to gradually reduce this learning rate. In this method which is used in multi-layer networks using back-propagation learning algorithm, network error is reduced in a short time using differential adaptive learning rate. ?? 2009 WASET.ORG.},
author = {Iranmanesh, Saeid and Mahdavi, M. Amin},
file = {:home/alkhaldieid/Desktop/papers/papers/Learning Rate Schedueling/A{\_}differential{\_}adaptive{\_}learning{\_}rate{\_}method{\_}for{\_}b.pdf:pdf},
issn = {2010376X},
journal = {World Academy of Science, Engineering and Technology},
keywords = {Activation function,Adaptive learning rate,Nguyen-Widrow method,Optical-BP,Standard BP},
number = {March},
pages = {289--292},
title = {{A differential adaptive learning rate method for back-propagation neural networks}},
volume = {38},
year = {2009}
}
@article{Lee2019,
abstract = {Pretrained transformer-based language models have achieved state of the art across countless tasks in natural language processing. These models are highly expressive, comprising at least a hundred million parameters and a dozen layers. Recent evidence suggests that only a few of the final layers need to be fine-tuned for high quality on downstream tasks. Naturally, a subsequent research question is, "how many of the last layers do we need to fine-tune?" In this paper, we precisely answer this question. We examine two recent pretrained language models, BERT and RoBERTa, across standard tasks in textual entailment, semantic similarity, sentiment analysis, and linguistic acceptability. We vary the number of final layers that are fine-tuned, then study the resulting change in task-specific effectiveness. We show that only a fourth of the final layers need to be fine-tuned to achieve 90{\%} of the original quality. Surprisingly, we also find that fine-tuning all layers does not always help.},
archivePrefix = {arXiv},
arxivId = {1911.03090},
author = {Lee, Jaejun and Tang, Raphael and Lin, Jimmy},
eprint = {1911.03090},
file = {:home/alkhaldieid/Desktop/papers/papers/Learning Rate Schedueling/What Would Elsa Do? Freezing Layers During Transformer Fine-Tuning.pdf:pdf},
title = {{What Would Elsa Do? Freezing Layers During Transformer Fine-Tuning}},
url = {http://arxiv.org/abs/1911.03090},
year = {2019}
}
@article{Thompson2019a,
abstract = {Characterization of the representations learned in intermediate layers of deep networks can provide valuable insight into the nature of a task and can guide the development of well-tailored learning strategies. Here we study convolutional neural network (CNN)-based acoustic models in the context of automatic speech recognition. Adapting a method proposed by [1], we measure the transferability of each layer between English, Dutch and German to assess their language-specificity. We observed three distinct regions of transferability: (1) the first two layers were entirely transferable between languages, (2) layers 2-8 were also highly transferable but we found some evidence of language specificity, (3) the subsequent fully connected layers were more language specific but could be successfully finetuned to the target language. To further probe the effect of weight freezing, we performed follow-up experiments using freeze-training [2]. Our results are consistent with the observation that CNNs converge 'bottom up' during training and demonstrate the benefit of freeze training, especially for transfer learning.},
author = {Thompson, Jessica A.F. and Schonwiesner, Marc and Bengio, Yoshua and Willett, Daniel},
doi = {10.1109/ICASSP.2019.8683043},
file = {:home/alkhaldieid/Desktop/papers/papers/Learning Rate Schedueling/how{\_}transferable{\_}are{\_}features{\_}in{\_}convolutional{\_}neural{\_}network{\_}acoustic{\_}models{\_}across{\_}languages{\_}.pdf:pdf},
isbn = {9781479981311},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {CNNs,acoustic modeling,freeze training,interpretability,language-specificity,transfer learning},
number = {Nips},
pages = {2827--2831},
title = {{How Transferable Are Features in Convolutional Neural Network Acoustic Models across Languages?}},
volume = {2019-May},
year = {2019}
}
@inproceedings{Zakerzadeh2014a,
abstract = {The curse of dimensionality has remained a challenge for a wide variety of algorithms in data mining, clustering, classification and privacy. Recently, it was shown that an increasing dimensionality makes the data resistant to effective privacy. The theoretical results seem to suggest that the dimensionality curse is a fundamental barrier to privacy preservation. However, in practice, we show that some of the common properties of real data can be leveraged in order to greatly ameliorate the negative effects of the curse of dimensionality. In real data sets, many dimensions contain high levels of inter-attribute correlations. Such correlations enable the use of a process known as vertical fragmentation in order to decompose the data into vertical subsets of smaller dimensionality. An information-theoretic criterion of mutual information is used in the vertical decomposition process. This allows the use of an anonymization process, which is based on combining results from multiple independent fragments. We present a general approach which can be applied to the k-anonymity, l-diversity, and t-closeness models. In the presence of inter-attribute correlations, such an approach continues to be much more robust in higher dimensionality, without losing accuracy. We present experimental results illustrating the effectiveness of the approach. This approach is resilient enough to prevent identity, attribute, and membership disclosure attack.},
archivePrefix = {arXiv},
arxivId = {1401.1174},
author = {Zakerzadeh, Hessam and Aggarwal, Charu C. and Barker, Ken},
booktitle = {SIAM International Conference on Data Mining 2014, SDM 2014},
doi = {10.1137/1.9781611973440.84},
eprint = {1401.1174},
file = {:home/alkhaldieid/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zakerzadeh, Aggarwal, Barker - 2014 - Towards breaking the curse of dimensionality for high-dimensional privacy.pdf:pdf},
isbn = {9781510811515},
pages = {731--739},
publisher = {Society for Industrial and Applied Mathematics Publications},
title = {{Towards breaking the curse of dimensionality for high-dimensional privacy}},
volume = {2},
year = {2014}
}
@article{Litjens2017,
abstract = {Deep learning algorithms, in particular convolutional networks, have rapidly become a methodology of choice for analyzing medical images. This paper reviews the major deep learning concepts pertinent to medical image analysis and summarizes over 300 contributions to the field, most of which appeared in the last year. We survey the use of deep learning for image classification, object detection, segmentation, registration, and other tasks. Concise overviews are provided of studies per application area: neuro, retinal, pulmonary, digital pathology, breast, cardiac, abdominal, musculoskeletal. We end with a summary of the current state-of-the-art, a critical discussion of open challenges and directions for future research.},
archivePrefix = {arXiv},
arxivId = {1702.05747},
author = {Litjens, Geert and Kooi, Thijs and Bejnordi, Babak Ehteshami and Setio, Arnaud Arindra Adiyoso and Ciompi, Francesco and Ghafoorian, Mohsen and van der Laak, Jeroen A.W.M. and van Ginneken, Bram and S{\'{a}}nchez, Clara I.},
doi = {10.1016/j.media.2017.07.005},
eprint = {1702.05747},
file = {:home/alkhaldieid/Desktop/papers/papers/TBD/survey on medical + deep.pdf:pdf},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Convolutional neural networks,Deep learning,Medical imaging,Survey},
number = {1995},
pages = {60--88},
pmid = {28778026},
title = {{A survey on deep learning in medical image analysis}},
volume = {42},
year = {2017}
}
@article{Sankaranarayanan2018,
abstract = {Domain Adaptation is an actively researched problem in Computer Vision. In this work, we propose an approach that leverages unsupervised data to bring the source and target distributions closer in a learned joint feature space. We accomplish this by inducing a symbiotic relationship between the learned embedding and a generative adversarial network. This is in contrast to methods which use the adversarial framework for realistic data generation and retraining deep models with such data. We demonstrate the strength and generality of our approach by performing experiments on three different tasks with varying levels of difficulty: (1) Digit classification (MNIST, SVHN and USPS datasets) (2) Object recognition using OFFICE dataset and (3) Domain adaptation from synthetic to real data. Our method achieves state-of-the art performance in most experimental settings and by far the only GAN-based method that has been shown to work well across different datasets such as OFFICE and DIGITS.},
archivePrefix = {arXiv},
arxivId = {arXiv:1704.01705v4},
author = {Sankaranarayanan, Swami and Balaji, Yogesh and Castillo, Carlos D. and Chellappa, Rama},
doi = {10.1109/CVPR.2018.00887},
eprint = {arXiv:1704.01705v4},
file = {:home/alkhaldieid/Desktop/papers/papers/gan/gan.pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {8503--8512},
title = {{Generate to Adapt: Aligning Domains Using Generative Adversarial Networks}},
year = {2018}
}
@article{Ferriss2007,
abstract = {What do you do? Tim Ferriss has trouble answering the question. Depending on when you ask this controversial Princeton University guest lecturer, he might answer: “I race motorcycles in Europe.” “I ski in the Andes.” “I scuba dive in Panama.” “I dance tango in Buenos Aires.” He has spent more than five years learning the secrets of the New Rich, a fast-growing subculture who has abandoned the “deferred-life plan” and instead mastered the new currencies—time and mobility—to create luxury lifestyles in the here and now. Whether you are an overworked employee or an entrepreneur trapped in your own business, this book is the compass for a new and revolutionary world. Join Tim Ferriss as he teaches you: • How to outsource your life to overseas virtual assistants for {\$}5 per hour and do whatever you want • How blue-chip escape artists travel the world without quitting their jobs • How to eliminate 50{\%} of your work in 48 hours using the principles of a forgotten Italian economist • How to trade a long-haul career for short work bursts and freuent "mini-retirements" • What the crucial difference is between absolute and relative income • How to train your boss to value performance over presence, or kill your job (or company) if it's beyond repair • What automated cash-flow “muses” are and how to create one in 2 to 4 weeks • How to cultivate selective ignorance—and create time—with a low-information diet • What the management secrets of Remote Control CEOs are • How to get free housing worldwide and airfare at 50–80{\%} off • How to fill the void and create a meaningful life after removing work and the office You can have it all—really.},
author = {Ferriss, Timothy},
file = {:home/alkhaldieid/Desktop/papers/How to write a good paper/FERRISS{\_}Timothy{\_}-{\_}The{\_}4-Hour{\_}Workweek.pdf:pdf},
isbn = {9780307465351},
pages = {416},
title = {{The 4-Hour Workweek (Expanded Edition)}},
url = {http://www.amazon.com/The-4-Hour-Workweek-Anywhere-Expanded/dp/0307465357/ref=sr{\_}1{\_}1?ie=UTF8{\&}qid=1384165883{\&}sr=8-1{\&}keywords=9780307465351},
year = {2007}
}
@book{Negnevitsky,
author = {Negnevitsky, Michael},
file = {:home/alkhaldieid/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Negnevitsky - Unknown - Artificial Intelligence.pdf:pdf},
isbn = {0321204662},
title = {{Artificial Intelligence}}
}
@inproceedings{Zakerzadeh2014b,
abstract = {The curse of dimensionality has remained a challenge for a wide variety of algorithms in data mining, clustering, classification and privacy. Recently, it was shown that an increasing dimensionality makes the data resistant to effective privacy. The theoretical results seem to suggest that the dimensionality curse is a fundamental barrier to privacy preservation. However, in practice, we show that some of the common properties of real data can be leveraged in order to greatly ameliorate the negative effects of the curse of dimensionality. In real data sets, many dimensions contain high levels of inter-attribute correlations. Such correlations enable the use of a process known as vertical fragmentation in order to decompose the data into vertical subsets of smaller dimensionality. An information-theoretic criterion of mutual information is used in the vertical decomposition process. This allows the use of an anonymization process, which is based on combining results from multiple independent fragments. We present a general approach which can be applied to the k-anonymity, l-diversity, and t-closeness models. In the presence of inter-attribute correlations, such an approach continues to be much more robust in higher dimensionality, without losing accuracy. We present experimental results illustrating the effectiveness of the approach. This approach is resilient enough to prevent identity, attribute, and membership disclosure attack.},
archivePrefix = {arXiv},
arxivId = {1401.1174},
author = {Zakerzadeh, Hessam and Aggarwal, Charu C. and Barker, Ken},
booktitle = {SIAM International Conference on Data Mining 2014, SDM 2014},
doi = {10.1137/1.9781611973440.84},
eprint = {1401.1174},
file = {:home/alkhaldieid/Desktop/papers/papers/1401.1174curse:1174curse},
isbn = {9781510811515},
pages = {731--739},
title = {{Towards breaking the curse of dimensionality for high-dimensional privacy}},
url = {http://arxiv.org/abs/1401.1174},
volume = {2},
year = {2014}
}
@article{Lin2013,
abstract = {We propose a novel deep network structure called "Network In Network" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.},
archivePrefix = {arXiv},
arxivId = {1312.4400},
author = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
eprint = {1312.4400},
file = {:home/alkhaldieid/Desktop/papers/papers/TBD/Network In Network.pdf:pdf},
pages = {1--10},
title = {{Network In Network}},
url = {http://arxiv.org/abs/1312.4400},
year = {2013}
}

@Comment{jabref-meta: databaseType:bibtex;}
